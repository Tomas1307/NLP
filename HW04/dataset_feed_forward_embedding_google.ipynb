{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset creation for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow\n",
    "#!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from utils_processor.processor import Processor\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import gensim\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers  # Añadimos esta importación\n",
    "from gensim.models import Word2Vec\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers  # Añadimos esta importaciónimport tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras.backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor_ = Processor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando archivo: ATaleofTwoCities_Dickens.txt, lineas:  776878\n",
      "Procesando archivo: BleakHouse_Dickens.txt, lineas:  1958792\n",
      "Procesando archivo: CountofMonteCristo_Dumas.txt, lineas:  2646641\n",
      "Procesando archivo: CrimeAndPunishment_dostoyevski.txt, lineas:  1154409\n",
      "Procesando archivo: OliverTwist_Dickens.txt, lineas:  912421\n",
      "Procesando archivo: TheGambler_dostoyevski.txt, lineas:  350954\n",
      "Procesando archivo: TheIdiot_dostoyevski.txt, lineas:  1366983\n",
      "Procesando archivo: TheThreeMusketeers_Dumas.txt, lineas:  1317339\n",
      "Procesando archivo: TwentyYearsAfter_Dumas.txt, lineas:  1387344\n",
      "Lista de autores en orden:\n",
      "1. charles dickens\n",
      "2. charles dickens\n",
      "3. alexandre dumas\n",
      "4. fyodor dostoyevsky\n",
      "5. charles dickens\n",
      "6. fyodor dostoyevsky\n",
      "7. fyodor dostoyevsky\n",
      "8. alexandre dumas\n",
      "9. alexandre dumas\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['charles dickens',\n",
       " 'charles dickens',\n",
       " 'alexandre dumas',\n",
       " 'fyodor dostoyevsky',\n",
       " 'charles dickens',\n",
       " 'fyodor dostoyevsky',\n",
       " 'fyodor dostoyevsky',\n",
       " 'alexandre dumas',\n",
       " 'alexandre dumas']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "# Directorio donde están los archivos .txt\n",
    "data_dir = 'data/books/'\n",
    "\n",
    "# Lista para almacenar los textos y autores\n",
    "texts = []\n",
    "authors = []\n",
    "\n",
    "# Leer todos los archivos .txt del directorio\n",
    "for filename in os.listdir(data_dir):\n",
    "    if filename.endswith('.txt'):\n",
    "\n",
    "        with open(os.path.join(data_dir, filename), 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "            texts.append(text)  # Almacenar el texto\n",
    "            print(f\"Procesando archivo: {filename}, lineas: \", len(text))\n",
    "            \n",
    "            # Buscar el nombre del autor\n",
    "            author_match = re.search(r'Author:\\s*(.+)', text)\n",
    "            if author_match:\n",
    "                author_name = author_match.group(1).strip()\n",
    "                authors.append(author_name.lower())  # Almacenar el autor\n",
    "            else:\n",
    "                authors.append(\"Autor no encontrado\")  # En caso de no encontrarlo\n",
    "            \n",
    "            # Mostrar un fragmento del texto (opcional)\n",
    "            #print(text[:2500])\n",
    "\n",
    "# Mostrar los autores encontrados\n",
    "print(\"Lista de autores en orden:\")\n",
    "for i, author in enumerate(authors):\n",
    "    print(f\"{i+1}. {author}\")\n",
    "\n",
    "# Ahora tienes dos listas: 'texts' con los textos y 'authors' con los autores en el mismo orden\n",
    "authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_texts(processor = Processor(), texts: list = []):\n",
    "    \"\"\"\n",
    "    Processes a list of texts and logs progress for each one, using the Processor class.\n",
    "    \n",
    "    Args:\n",
    "        processor (Processor): An instance of the Processor class.\n",
    "        texts (list): A list of text strings to process.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of processed texts.\n",
    "    \"\"\"\n",
    "    total = len(texts)\n",
    "    processed_texts = []\n",
    "    \n",
    "    for index, text in enumerate(texts):\n",
    "        processed_text = processor.preprocessing_pipeline_as_chunks(text, index, total)\n",
    "        processed_texts.append(processed_text)  # Guardamos el texto procesado como lista de tokens\n",
    "    \n",
    "    return processed_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procesar todos los textos con el sistema de logging\n",
    "processed_texts = process_all_texts(processor_, texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks = []\n",
    "chunk_authors = []\n",
    "\n",
    "for i, text_list in enumerate(processed_texts):\n",
    "    author = authors[i]\n",
    "    for chunk in text_list:\n",
    "        text_chunks.append(chunk)  # Agregar cada chunk de texto\n",
    "        chunk_authors.append(author)  # Agregar el autor correspondiente\n",
    "\n",
    "# Crear un DataFrame con las listas\n",
    "df_chunks = pd.DataFrame({\n",
    "    'text_chunk': text_chunks,\n",
    "    'author': chunk_authors\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_chunk</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tale num citi tale num citi stori french revol...</td>\n",
       "      <td>charles dickens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chapter xv knit chapter xvi still knit chapter...</td>\n",
       "      <td>charles dickens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>life chapter period best time worst time age w...</td>\n",
       "      <td>charles dickens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>king larg jaw queen plain face throne england ...</td>\n",
       "      <td>charles dickens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>london westminst even cocklan ghost laid round...</td>\n",
       "      <td>charles dickens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16890</th>\n",
       "      <td>critic reach project gutenberg goal ensur proj...</td>\n",
       "      <td>alexandre dumas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16891</th>\n",
       "      <td>num contribut project gutenberg literari archi...</td>\n",
       "      <td>alexandre dumas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16892</th>\n",
       "      <td>num num particular import maintain tax exempt ...</td>\n",
       "      <td>alexandre dumas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16893</th>\n",
       "      <td>us offer donat intern donat grate accept make ...</td>\n",
       "      <td>alexandre dumas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16894</th>\n",
       "      <td>often creat sever print edit confirm protect c...</td>\n",
       "      <td>alexandre dumas</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16895 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              text_chunk           author\n",
       "0      tale num citi tale num citi stori french revol...  charles dickens\n",
       "1      chapter xv knit chapter xvi still knit chapter...  charles dickens\n",
       "2      life chapter period best time worst time age w...  charles dickens\n",
       "3      king larg jaw queen plain face throne england ...  charles dickens\n",
       "4      london westminst even cocklan ghost laid round...  charles dickens\n",
       "...                                                  ...              ...\n",
       "16890  critic reach project gutenberg goal ensur proj...  alexandre dumas\n",
       "16891  num contribut project gutenberg literari archi...  alexandre dumas\n",
       "16892  num num particular import maintain tax exempt ...  alexandre dumas\n",
       "16893  us offer donat intern donat grate accept make ...  alexandre dumas\n",
       "16894  often creat sever print edit confirm protect c...  alexandre dumas\n",
       "\n",
       "[16895 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Prepare the text data\n",
    "\n",
    "We already have the processed texts stored in a list called processed_texts. Each element in this list represents the chunks of text (after splitting) for a particular book.\n",
    "Each entry in processed_texts is a list where each element is a chunk of text for that book, processed based on the method we implemented for splitting into chunks of 150 words with a 25-word overlap\n",
    "\n",
    "2. Prepare the author labels\n",
    "\n",
    "We have an authors list that stores the corresponding author for each book in processed_texts. Each author appears multiple times if they have multiple books in the dataset. For example:\n",
    "python\n",
    "\n",
    "authors = ['dostoyevski', 'poe', 'dostoyevski', 'dostoyevski', 'well', 'poe', 'poe', 'well', 'well']\n",
    "\n",
    "3. Create the DataFrame structure\n",
    "\n",
    "For each processed book (i.e., processed_texts[i]), we know that all the chunks of that book correspond to a specific author. So we can assign the same author to all the chunks in that list.\n",
    "We will loop over each entry in processed_texts and for each chunk, add it to a DataFrame, along with the corresponding author.\n",
    "\n",
    "4. Steps to build the DataFrame\n",
    "\n",
    "* Initialize lists for the DataFrame: We will initialize two lists: one for text chunks and one for authors.\n",
    "* Iterate over processed_texts: For each entry in processed_texts, we extract the list of chunks and the corresponding author.\n",
    "* Add chunks and authors to the lists: For each chunk in the list of text chunks, we append it to the \"text_chunk\" list and the corresponding author to the \"author\" list.\n",
    "* Create the DataFrame: Once the lists are filled, we create a pandas DataFrame with two columns: \"text_chunk\" and \"author\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming df_chunks is the dataframe with the columns ['text_chunk', 'author']\n",
    "\n",
    "# Step 1: Split the dataset into 70% training and 30% test\n",
    "train_df, test_df = train_test_split(df_chunks, test_size=0.30, stratify=df_chunks['author'], random_state=42)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.1, stratify=train_df['author'], random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_chunk</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10344</th>\n",
       "      <td>elbow beg pardon said dodger look air abstract...</td>\n",
       "      <td>charles dickens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2552</th>\n",
       "      <td>pursu say emphat william guppi drop mr guppi a...</td>\n",
       "      <td>charles dickens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13609</th>\n",
       "      <td>secret use know anyth said young woman instinc...</td>\n",
       "      <td>alexandre dumas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3369</th>\n",
       "      <td>look keep secret condescens present visit feel...</td>\n",
       "      <td>charles dickens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9896</th>\n",
       "      <td>jew sooner alon counten resum former express a...</td>\n",
       "      <td>charles dickens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5799</th>\n",
       "      <td>better inform know owner hors shut cri pit cho...</td>\n",
       "      <td>alexandre dumas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8183</th>\n",
       "      <td>began count third excus would say fanci made m...</td>\n",
       "      <td>fyodor dostoyevsky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13251</th>\n",
       "      <td>take away commiss give mademoisell de chemerau...</td>\n",
       "      <td>alexandre dumas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4851</th>\n",
       "      <td>must curios natur island mass rock contain acr...</td>\n",
       "      <td>alexandre dumas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9704</th>\n",
       "      <td>harden littl wretch take away said mr bumbl im...</td>\n",
       "      <td>charles dickens</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10643 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              text_chunk              author\n",
       "10344  elbow beg pardon said dodger look air abstract...     charles dickens\n",
       "2552   pursu say emphat william guppi drop mr guppi a...     charles dickens\n",
       "13609  secret use know anyth said young woman instinc...     alexandre dumas\n",
       "3369   look keep secret condescens present visit feel...     charles dickens\n",
       "9896   jew sooner alon counten resum former express a...     charles dickens\n",
       "...                                                  ...                 ...\n",
       "5799   better inform know owner hors shut cri pit cho...     alexandre dumas\n",
       "8183   began count third excus would say fanci made m...  fyodor dostoyevsky\n",
       "13251  take away commiss give mademoisell de chemerau...     alexandre dumas\n",
       "4851   must curios natur island mass rock contain acr...     alexandre dumas\n",
       "9704   harden littl wretch take away said mr bumbl im...     charles dickens\n",
       "\n",
       "[10643 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_chunk</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5881</th>\n",
       "      <td>deceiv play joke excel read ah true said mont ...</td>\n",
       "      <td>alexandre dumas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>heel head wish wos still say prewar sir let bo...</td>\n",
       "      <td>charles dickens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16471</th>\n",
       "      <td>would choos num atho artagnan said noth silenc...</td>\n",
       "      <td>alexandre dumas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3294</th>\n",
       "      <td>littl earlier morn keep account attend houseke...</td>\n",
       "      <td>charles dickens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13959</th>\n",
       "      <td>shall get back upon lackey hors _pardieu_ anyb...</td>\n",
       "      <td>alexandre dumas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8226</th>\n",
       "      <td>heart long anoth father polya papa fear angri ...</td>\n",
       "      <td>fyodor dostoyevsky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11409</th>\n",
       "      <td>note often grow paler take princ took note fer...</td>\n",
       "      <td>fyodor dostoyevsky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7376</th>\n",
       "      <td>room first floor room whitewash custom prison ...</td>\n",
       "      <td>alexandre dumas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1898</th>\n",
       "      <td>time littl woman ad rub head signific settl ye...</td>\n",
       "      <td>charles dickens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2662</th>\n",
       "      <td>deal amount vehem make mind speak final finish...</td>\n",
       "      <td>charles dickens</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1183 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              text_chunk              author\n",
       "5881   deceiv play joke excel read ah true said mont ...     alexandre dumas\n",
       "882    heel head wish wos still say prewar sir let bo...     charles dickens\n",
       "16471  would choos num atho artagnan said noth silenc...     alexandre dumas\n",
       "3294   littl earlier morn keep account attend houseke...     charles dickens\n",
       "13959  shall get back upon lackey hors _pardieu_ anyb...     alexandre dumas\n",
       "...                                                  ...                 ...\n",
       "8226   heart long anoth father polya papa fear angri ...  fyodor dostoyevsky\n",
       "11409  note often grow paler take princ took note fer...  fyodor dostoyevsky\n",
       "7376   room first floor room whitewash custom prison ...     alexandre dumas\n",
       "1898   time littl woman ad rub head signific settl ye...     charles dickens\n",
       "2662   deal amount vehem make mind speak final finish...     charles dickens\n",
       "\n",
       "[1183 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_chunk</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12339</th>\n",
       "      <td>quit sure reach culmin point happi num day saw...</td>\n",
       "      <td>fyodor dostoyevsky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4185</th>\n",
       "      <td>spain itali mercédè father could join fear liv...</td>\n",
       "      <td>alexandre dumas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15378</th>\n",
       "      <td>found pale fatigu inquir whether ill fact said...</td>\n",
       "      <td>alexandre dumas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9926</th>\n",
       "      <td>empti comfort said mrs corney much inde said b...</td>\n",
       "      <td>charles dickens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6920</th>\n",
       "      <td>crush singl touch word breath yes self thought...</td>\n",
       "      <td>alexandre dumas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2448</th>\n",
       "      <td>poor dear girl found much admir good disposit ...</td>\n",
       "      <td>charles dickens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14473</th>\n",
       "      <td>dispos convers reclin corner carriag num pass ...</td>\n",
       "      <td>alexandre dumas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14223</th>\n",
       "      <td>smile indic knew stori well wish relat recomme...</td>\n",
       "      <td>alexandre dumas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16470</th>\n",
       "      <td>shall begin portho arami drew back disappoint ...</td>\n",
       "      <td>alexandre dumas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>slowli shook shadow turn shall go home father ...</td>\n",
       "      <td>charles dickens</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5069 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              text_chunk              author\n",
       "12339  quit sure reach culmin point happi num day saw...  fyodor dostoyevsky\n",
       "4185   spain itali mercédè father could join fear liv...     alexandre dumas\n",
       "15378  found pale fatigu inquir whether ill fact said...     alexandre dumas\n",
       "9926   empti comfort said mrs corney much inde said b...     charles dickens\n",
       "6920   crush singl touch word breath yes self thought...     alexandre dumas\n",
       "...                                                  ...                 ...\n",
       "2448   poor dear girl found much admir good disposit ...     charles dickens\n",
       "14473  dispos convers reclin corner carriag num pass ...     alexandre dumas\n",
       "14223  smile indic knew stori well wish relat recomme...     alexandre dumas\n",
       "16470  shall begin portho arami drew back disappoint ...     alexandre dumas\n",
       "223    slowli shook shadow turn shall go home father ...     charles dickens\n",
       "\n",
       "[5069 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def summary_by_author(train_df, validation_df, test_df):\n",
    "    \"\"\"\n",
    "    Generates a summary table showing the number of samples per author for the training, validation, and testing sets.\n",
    "    \n",
    "    Args:\n",
    "        train_df (pd.DataFrame): Training DataFrame.\n",
    "        validation_df (pd.DataFrame): Validation DataFrame.\n",
    "        test_df (pd.DataFrame): Testing DataFrame.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: A summary DataFrame.\n",
    "    \"\"\"\n",
    "    \n",
    "    summary_data = {\n",
    "        'Author': train_df['author'].unique(),\n",
    "        'Train': train_df['author'].value_counts(),\n",
    "        'Validation': validation_df['author'].value_counts(),\n",
    "        'Test': test_df['author'].value_counts()\n",
    "    }\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    summary_df = summary_df.fillna(0)  # Replace NaN with 0 if no samples exist for some authors\n",
    "    \n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Train</th>\n",
       "      <th>Validation</th>\n",
       "      <th>Test</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>author</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>alexandre dumas</th>\n",
       "      <td>charles dickens</td>\n",
       "      <td>4744</td>\n",
       "      <td>527</td>\n",
       "      <td>2260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>charles dickens</th>\n",
       "      <td>alexandre dumas</td>\n",
       "      <td>3307</td>\n",
       "      <td>368</td>\n",
       "      <td>1575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fyodor dostoyevsky</th>\n",
       "      <td>fyodor dostoyevsky</td>\n",
       "      <td>2592</td>\n",
       "      <td>288</td>\n",
       "      <td>1234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Author  Train  Validation  Test\n",
       "author                                                         \n",
       "alexandre dumas        charles dickens   4744         527  2260\n",
       "charles dickens        alexandre dumas   3307         368  1575\n",
       "fyodor dostoyevsky  fyodor dostoyevsky   2592         288  1234"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_by_author(train_df, val_df, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feed Forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec large embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el modelo de GoogleNews\n",
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('data/largest_embedding/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparando datos de entrenamiento...\n",
      "Procesando textos...\n",
      "Palabras únicas en datos: 22297\n",
      "Palabras encontradas en word2vec: 8416\n",
      "Tamaño final del vocabulario (incluyendo PAD y UNK): 8418\n",
      "\n",
      "Preparando datos de validación...\n",
      "Procesando textos...\n",
      "Palabras únicas en datos: 8788\n",
      "Palabras encontradas en word2vec: 4754\n",
      "Tamaño final del vocabulario (incluyendo PAD y UNK): 4756\n",
      "\n",
      "Preparando datos de prueba...\n",
      "Procesando textos...\n",
      "Palabras únicas en datos: 16474\n",
      "Palabras encontradas en word2vec: 7121\n",
      "Tamaño final del vocabulario (incluyendo PAD y UNK): 7123\n",
      "\n",
      "Aplicando padding...\n",
      "Longitud máxima de secuencia: 121\n",
      "\n",
      "Creando matriz de embeddings...\n"
     ]
    }
   ],
   "source": [
    "def prepare_data(df, processor_, word2vec_model):\n",
    "    X = []\n",
    "    print(\"Procesando textos...\")\n",
    "    \n",
    "    # Definir índices especiales\n",
    "    PAD_IDX = 0\n",
    "    UNK_IDX = 1\n",
    "    \n",
    "    # Primero, encontrar todas las palabras únicas en nuestros datos\n",
    "    unique_words = set()\n",
    "    for text in df['text_chunk']:\n",
    "        processed_tokens = processor_.preprocessing_pipeline(text)\n",
    "        unique_words.update(processed_tokens)\n",
    "    \n",
    "    # Crear un vocabulario limitado solo a las palabras que aparecen en nuestros datos\n",
    "    # y que también están en word2vec\n",
    "    valid_words = {word for word in unique_words if word in word2vec_model.key_to_index}\n",
    "    \n",
    "    # Crear un nuevo mapeo de índices\n",
    "    word_to_idx = {word: (idx + 2) for idx, word in enumerate(valid_words)}\n",
    "    vocab_size = len(word_to_idx) + 2  # +2 para PAD y UNK\n",
    "    \n",
    "    print(f\"Palabras únicas en datos: {len(unique_words)}\")\n",
    "    print(f\"Palabras encontradas en word2vec: {len(valid_words)}\")\n",
    "    print(f\"Tamaño final del vocabulario (incluyendo PAD y UNK): {vocab_size}\")\n",
    "    \n",
    "    # Procesar los textos con el nuevo vocabulario\n",
    "    for text in df['text_chunk']:\n",
    "        processed_tokens = processor_.preprocessing_pipeline(text)\n",
    "        indices = [word_to_idx.get(token, UNK_IDX) for token in processed_tokens]\n",
    "        X.append(indices)\n",
    "    \n",
    "    return X, vocab_size, word_to_idx\n",
    "\n",
    "def create_embedding_matrix(word2vec_model, word_to_idx, embedding_size):\n",
    "    vocab_size = len(word_to_idx) + 2  # +2 para PAD y UNK\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_size))\n",
    "    \n",
    "    # Vector para PAD (todos ceros)\n",
    "    embedding_matrix[0] = np.zeros(embedding_size)\n",
    "    \n",
    "    # Vector para UNK (promedio de todos los vectores de palabras válidas)\n",
    "    valid_vectors = [word2vec_model[word] for word in word_to_idx.keys()]\n",
    "    embedding_matrix[1] = np.mean(valid_vectors, axis=0)\n",
    "    \n",
    "    # Llenar vectores para palabras del vocabulario\n",
    "    for word, idx in word_to_idx.items():\n",
    "        embedding_matrix[idx] = word2vec_model[word]\n",
    "    \n",
    "    return embedding_matrix\n",
    "\n",
    "# Preparar los datos\n",
    "print(\"Preparando datos de entrenamiento...\")\n",
    "X_train, vocab_size, word_to_idx = prepare_data(train_df, processor_, word2vec_model)\n",
    "print(\"\\nPreparando datos de validación...\")\n",
    "X_val, _, _ = prepare_data(val_df, processor_, word2vec_model)\n",
    "print(\"\\nPreparando datos de prueba...\")\n",
    "X_test, _, _ = prepare_data(test_df, processor_, word2vec_model)\n",
    "\n",
    "# Padding\n",
    "print(\"\\nAplicando padding...\")\n",
    "max_length = max(\n",
    "    max(len(x) for x in X_train),\n",
    "    max(len(x) for x in X_val),\n",
    "    max(len(x) for x in X_test)\n",
    ")\n",
    "print(f\"Longitud máxima de secuencia: {max_length}\")\n",
    "\n",
    "X_train_padded = pad_sequences(X_train, maxlen=max_length, padding='post')\n",
    "X_val_padded = pad_sequences(X_val, maxlen=max_length, padding='post')\n",
    "X_test_padded = pad_sequences(X_test, maxlen=max_length, padding='post')\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(train_df['author'])\n",
    "\n",
    "y_train = label_encoder.transform(train_df['author'])\n",
    "y_val = label_encoder.transform(val_df['author'])\n",
    "y_test = label_encoder.transform(test_df['author'])\n",
    "\n",
    "classes = np.unique(y_train)\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=classes,\n",
    "    y=y_train\n",
    ")\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "y_train_cat = to_categorical(y_train)\n",
    "y_val_cat = to_categorical(y_val)\n",
    "y_test_cat = to_categorical(y_test)\n",
    "\n",
    "\n",
    "# Crear matriz de embeddings\n",
    "print(\"\\nCreando matriz de embeddings...\")\n",
    "embedding_matrix = create_embedding_matrix(word2vec_model, word_to_idx, embedding_size=300)\n",
    "\n",
    "# Crear y entrenar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forma y tipo de las etiquetas:\n",
      "y_train shape: (10643, 3)\n",
      "Valores únicos en y_val: [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]]\n",
      "Valores únicos en y_test: [0. 1.]\n",
      "Valores únicos en y_train: [0. 1.]\n",
      "Tipo de datos de y_train: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Forma y tipo de las etiquetas:\")\n",
    "print(\"y_train shape:\", y_train_cat.shape)\n",
    "print(\"Valores únicos en y_val:\", y_val_cat)\n",
    "print(\"Valores únicos en y_test:\", np.unique(y_test_cat))\n",
    "print(\"Valores únicos en y_train:\", np.unique(y_train_cat))\n",
    "print(\"Tipo de datos de y_train:\", y_train_cat.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"best_model.h5\"  # Donde se guardará el mejor modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creando modelo...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'keras.src.activations' has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCreando modelo...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_latest_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Ya que usas GoogleNews que tiene 300 dimensiones\u001b[39;49;00m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_matrix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding_matrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Para las 3 categorías que mencionas\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Verificación final\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mVerificación final de índices:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[32], line 22\u001b[0m, in \u001b[0;36mcreate_latest_model\u001b[1;34m(vocab_size, embedding_dim, embedding_matrix, max_length, n_classes)\u001b[0m\n\u001b[0;32m     19\u001b[0m conv_blocks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Path 1: kernel_size = 3\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m conv1 \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConv1D\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msame\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m     27\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m(embedding)\n\u001b[0;32m     28\u001b[0m pool1 \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mMaxPooling1D(pool_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)(conv1)\n\u001b[0;32m     29\u001b[0m conv_blocks\u001b[38;5;241m.\u001b[39mappend(pool1)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\convolutional\\conv1d.py:115\u001b[0m, in \u001b[0;36mConv1D.__init__\u001b[1;34m(self, filters, kernel_size, strides, padding, data_format, dilation_rate, groups, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     97\u001b[0m     filters,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    114\u001b[0m ):\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrides\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdilation_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdilation_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mactivation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkernel_initializer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkernel_initializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbias_initializer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbias_initializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkernel_regularizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkernel_regularizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbias_regularizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbias_regularizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mactivity_regularizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mactivity_regularizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkernel_constraint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkernel_constraint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbias_constraint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbias_constraint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:118\u001b[0m, in \u001b[0;36mBaseConv.__init__\u001b[1;34m(self, rank, filters, kernel_size, strides, padding, data_format, dilation_rate, groups, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, lora_rank, **kwargs)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding \u001b[38;5;241m=\u001b[39m standardize_padding(padding, allow_causal\u001b[38;5;241m=\u001b[39mrank \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_format \u001b[38;5;241m=\u001b[39m standardize_data_format(data_format)\n\u001b[1;32m--> 118\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation \u001b[38;5;241m=\u001b[39m \u001b[43mactivations\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m(activation)\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_bias \u001b[38;5;241m=\u001b[39m use_bias\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_initializer \u001b[38;5;241m=\u001b[39m initializers\u001b[38;5;241m.\u001b[39mget(kernel_initializer)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'keras.src.activations' has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "print(\"\\nCreando modelo...\")\n",
    "model = create_latest_model(\n",
    "    vocab_size=vocab_size, \n",
    "    embedding_dim=300,  # Ya que usas GoogleNews que tiene 300 dimensiones\n",
    "    embedding_matrix=embedding_matrix,\n",
    "    max_length=max_length,\n",
    "    n_classes=3  # Para las 3 categorías que mencionas\n",
    ")\n",
    "\n",
    "# Verificación final\n",
    "print(\"\\nVerificación final de índices:\")\n",
    "print(f\"Rango válido: [0, {vocab_size})\")\n",
    "print(f\"Máximo en train: {X_train_padded.max()}\")\n",
    "print(f\"Máximo en val: {X_val_padded.max()}\")\n",
    "print(f\"Máximo en test: {X_test_padded.max()}\")\n",
    "\n",
    "# Entrenar el modelo\n",
    "history = model.fit(\n",
    "    X_train_padded, \n",
    "    y_train_cat,\n",
    "    validation_data=(X_val_padded, y_val_cat),\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=create_latest_callbacks(checkpoint_path),\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.2515 - loss: 1.0989\n",
      "\n",
      "Test accuracy: 0.2436\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = simplified_model.evaluate(X_test_padded, y_test)\n",
    "print(f\"\\nTest accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step\n",
      "\n",
      "Classification Report:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "   alexandre dumas       1.00      0.00      0.00      2260\n",
      "   charles dickens       0.00      0.00      0.00      1575\n",
      "fyodor dostoyevsky       0.24      1.00      0.39      1234\n",
      "\n",
      "          accuracy                           0.24      5069\n",
      "         macro avg       0.41      0.33      0.13      5069\n",
      "      weighted avg       0.51      0.24      0.10      5069\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USUARIO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\USUARIO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\USUARIO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Generate classification report\n",
    "y_pred = simplified_model.predict(X_test_padded)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_classes, target_names=label_encoder.classes_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
