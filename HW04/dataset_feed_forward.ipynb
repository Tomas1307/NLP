{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset creation for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from utils_processor.processor import Processor\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor_ = Processor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando archivo: ATaleofTwoCities_Dickens.txt, lineas:  776878\n",
      "Procesando archivo: BleakHouse_Dickens.txt, lineas:  1958792\n",
      "Procesando archivo: CountofMonteCristo_Dumas.txt, lineas:  2646641\n",
      "Procesando archivo: CrimeAndPunishment_dostoyevski.txt, lineas:  1154409\n",
      "Procesando archivo: OliverTwist_Dickens.txt, lineas:  912421\n",
      "Procesando archivo: TheGambler_dostoyevski.txt, lineas:  350954\n",
      "Procesando archivo: TheIdiot_dostoyevski.txt, lineas:  1366983\n",
      "Procesando archivo: TheThreeMusketeers_Dumas.txt, lineas:  1317339\n",
      "Procesando archivo: TwentyYearsAfter_Dumas.txt, lineas:  1387344\n",
      "Lista de autores en orden:\n",
      "1. charles dickens\n",
      "2. charles dickens\n",
      "3. alexandre dumas\n",
      "4. fyodor dostoyevsky\n",
      "5. charles dickens\n",
      "6. fyodor dostoyevsky\n",
      "7. fyodor dostoyevsky\n",
      "8. alexandre dumas\n",
      "9. alexandre dumas\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['charles dickens',\n",
       " 'charles dickens',\n",
       " 'alexandre dumas',\n",
       " 'fyodor dostoyevsky',\n",
       " 'charles dickens',\n",
       " 'fyodor dostoyevsky',\n",
       " 'fyodor dostoyevsky',\n",
       " 'alexandre dumas',\n",
       " 'alexandre dumas']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "# Directorio donde están los archivos .txt\n",
    "data_dir = 'data/books/'\n",
    "\n",
    "# Lista para almacenar los textos y autores\n",
    "texts = []\n",
    "authors = []\n",
    "\n",
    "# Leer todos los archivos .txt del directorio\n",
    "for filename in os.listdir(data_dir):\n",
    "    if filename.endswith('.txt'):\n",
    "\n",
    "        with open(os.path.join(data_dir, filename), 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "            texts.append(text)  # Almacenar el texto\n",
    "            print(f\"Procesando archivo: {filename}, lineas: \", len(text))\n",
    "            \n",
    "            # Buscar el nombre del autor\n",
    "            author_match = re.search(r'Author:\\s*(.+)', text)\n",
    "            if author_match:\n",
    "                author_name = author_match.group(1).strip()\n",
    "                authors.append(author_name.lower())  # Almacenar el autor\n",
    "            else:\n",
    "                authors.append(\"Autor no encontrado\")  # En caso de no encontrarlo\n",
    "            \n",
    "            # Mostrar un fragmento del texto (opcional)\n",
    "            #print(text[:2500])\n",
    "\n",
    "# Mostrar los autores encontrados\n",
    "print(\"Lista de autores en orden:\")\n",
    "for i, author in enumerate(authors):\n",
    "    print(f\"{i+1}. {author}\")\n",
    "\n",
    "# Ahora tienes dos listas: 'texts' con los textos y 'authors' con los autores en el mismo orden\n",
    "authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_texts(processor = Processor(), texts: list = []):\n",
    "    \"\"\"\n",
    "    Processes a list of texts and logs progress for each one, using the Processor class.\n",
    "    \n",
    "    Args:\n",
    "        processor (Processor): An instance of the Processor class.\n",
    "        texts (list): A list of text strings to process.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of processed texts.\n",
    "    \"\"\"\n",
    "    total = len(texts)\n",
    "    processed_texts = []\n",
    "    \n",
    "    for index, text in enumerate(texts):\n",
    "        processed_text = processor.preprocessing_pipeline_as_chunks(text, index, total)\n",
    "        processed_texts.append(processed_text)  # Guardamos el texto procesado como lista de tokens\n",
    "    \n",
    "    return processed_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procesar todos los textos con el sistema de logging\n",
    "processed_texts = process_all_texts(processor_, texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks = []\n",
    "chunk_authors = []\n",
    "\n",
    "for i, text_list in enumerate(processed_texts):\n",
    "    author = authors[i]\n",
    "    for chunk in text_list:\n",
    "        text_chunks.append(chunk)  # Agregar cada chunk de texto\n",
    "        chunk_authors.append(author)  # Agregar el autor correspondiente\n",
    "\n",
    "# Crear un DataFrame con las listas\n",
    "df_chunks = pd.DataFrame({\n",
    "    'text_chunk': text_chunks,\n",
    "    'author': chunk_authors\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_chunk</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tale num citi tale num citi stori french revol...</td>\n",
       "      <td>charles dickens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chapter xv knit chapter xvi still knit chapter...</td>\n",
       "      <td>charles dickens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>life chapter period best time worst time age w...</td>\n",
       "      <td>charles dickens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>king larg jaw queen plain face throne england ...</td>\n",
       "      <td>charles dickens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>london westminst even cocklan ghost laid round...</td>\n",
       "      <td>charles dickens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16890</th>\n",
       "      <td>critic reach project gutenberg goal ensur proj...</td>\n",
       "      <td>alexandre dumas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16891</th>\n",
       "      <td>num contribut project gutenberg literari archi...</td>\n",
       "      <td>alexandre dumas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16892</th>\n",
       "      <td>num num particular import maintain tax exempt ...</td>\n",
       "      <td>alexandre dumas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16893</th>\n",
       "      <td>us offer donat intern donat grate accept make ...</td>\n",
       "      <td>alexandre dumas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16894</th>\n",
       "      <td>often creat sever print edit confirm protect c...</td>\n",
       "      <td>alexandre dumas</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16895 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              text_chunk           author\n",
       "0      tale num citi tale num citi stori french revol...  charles dickens\n",
       "1      chapter xv knit chapter xvi still knit chapter...  charles dickens\n",
       "2      life chapter period best time worst time age w...  charles dickens\n",
       "3      king larg jaw queen plain face throne england ...  charles dickens\n",
       "4      london westminst even cocklan ghost laid round...  charles dickens\n",
       "...                                                  ...              ...\n",
       "16890  critic reach project gutenberg goal ensur proj...  alexandre dumas\n",
       "16891  num contribut project gutenberg literari archi...  alexandre dumas\n",
       "16892  num num particular import maintain tax exempt ...  alexandre dumas\n",
       "16893  us offer donat intern donat grate accept make ...  alexandre dumas\n",
       "16894  often creat sever print edit confirm protect c...  alexandre dumas\n",
       "\n",
       "[16895 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Prepare the text data\n",
    "\n",
    "We already have the processed texts stored in a list called processed_texts. Each element in this list represents the chunks of text (after splitting) for a particular book.\n",
    "Each entry in processed_texts is a list where each element is a chunk of text for that book, processed based on the method we implemented for splitting into chunks of 150 words with a 25-word overlap\n",
    "\n",
    "2. Prepare the author labels\n",
    "\n",
    "We have an authors list that stores the corresponding author for each book in processed_texts. Each author appears multiple times if they have multiple books in the dataset. For example:\n",
    "python\n",
    "\n",
    "authors = ['dostoyevski', 'poe', 'dostoyevski', 'dostoyevski', 'well', 'poe', 'poe', 'well', 'well']\n",
    "\n",
    "3. Create the DataFrame structure\n",
    "\n",
    "For each processed book (i.e., processed_texts[i]), we know that all the chunks of that book correspond to a specific author. So we can assign the same author to all the chunks in that list.\n",
    "We will loop over each entry in processed_texts and for each chunk, add it to a DataFrame, along with the corresponding author.\n",
    "\n",
    "4. Steps to build the DataFrame\n",
    "\n",
    "* Initialize lists for the DataFrame: We will initialize two lists: one for text chunks and one for authors.\n",
    "* Iterate over processed_texts: For each entry in processed_texts, we extract the list of chunks and the corresponding author.\n",
    "* Add chunks and authors to the lists: For each chunk in the list of text chunks, we append it to the \"text_chunk\" list and the corresponding author to the \"author\" list.\n",
    "* Create the DataFrame: Once the lists are filled, we create a pandas DataFrame with two columns: \"text_chunk\" and \"author\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming df_chunks is the dataframe with the columns ['text_chunk', 'author']\n",
    "\n",
    "# Step 1: Split the dataset into 70% training and 30% test\n",
    "train_df, test_df = train_test_split(df_chunks, test_size=0.30, stratify=df_chunks['author'], random_state=42)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.1, stratify=train_df['author'], random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_chunk</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10344</th>\n",
       "      <td>elbow beg pardon said dodger look air abstract...</td>\n",
       "      <td>charles dickens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2552</th>\n",
       "      <td>pursu say emphat william guppi drop mr guppi a...</td>\n",
       "      <td>charles dickens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13609</th>\n",
       "      <td>secret use know anyth said young woman instinc...</td>\n",
       "      <td>alexandre dumas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3369</th>\n",
       "      <td>look keep secret condescens present visit feel...</td>\n",
       "      <td>charles dickens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9896</th>\n",
       "      <td>jew sooner alon counten resum former express a...</td>\n",
       "      <td>charles dickens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5799</th>\n",
       "      <td>better inform know owner hors shut cri pit cho...</td>\n",
       "      <td>alexandre dumas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8183</th>\n",
       "      <td>began count third excus would say fanci made m...</td>\n",
       "      <td>fyodor dostoyevsky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13251</th>\n",
       "      <td>take away commiss give mademoisell de chemerau...</td>\n",
       "      <td>alexandre dumas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4851</th>\n",
       "      <td>must curios natur island mass rock contain acr...</td>\n",
       "      <td>alexandre dumas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9704</th>\n",
       "      <td>harden littl wretch take away said mr bumbl im...</td>\n",
       "      <td>charles dickens</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10643 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              text_chunk              author\n",
       "10344  elbow beg pardon said dodger look air abstract...     charles dickens\n",
       "2552   pursu say emphat william guppi drop mr guppi a...     charles dickens\n",
       "13609  secret use know anyth said young woman instinc...     alexandre dumas\n",
       "3369   look keep secret condescens present visit feel...     charles dickens\n",
       "9896   jew sooner alon counten resum former express a...     charles dickens\n",
       "...                                                  ...                 ...\n",
       "5799   better inform know owner hors shut cri pit cho...     alexandre dumas\n",
       "8183   began count third excus would say fanci made m...  fyodor dostoyevsky\n",
       "13251  take away commiss give mademoisell de chemerau...     alexandre dumas\n",
       "4851   must curios natur island mass rock contain acr...     alexandre dumas\n",
       "9704   harden littl wretch take away said mr bumbl im...     charles dickens\n",
       "\n",
       "[10643 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_chunk</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5881</th>\n",
       "      <td>deceiv play joke excel read ah true said mont ...</td>\n",
       "      <td>alexandre dumas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>heel head wish wos still say prewar sir let bo...</td>\n",
       "      <td>charles dickens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16471</th>\n",
       "      <td>would choos num atho artagnan said noth silenc...</td>\n",
       "      <td>alexandre dumas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3294</th>\n",
       "      <td>littl earlier morn keep account attend houseke...</td>\n",
       "      <td>charles dickens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13959</th>\n",
       "      <td>shall get back upon lackey hors _pardieu_ anyb...</td>\n",
       "      <td>alexandre dumas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8226</th>\n",
       "      <td>heart long anoth father polya papa fear angri ...</td>\n",
       "      <td>fyodor dostoyevsky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11409</th>\n",
       "      <td>note often grow paler take princ took note fer...</td>\n",
       "      <td>fyodor dostoyevsky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7376</th>\n",
       "      <td>room first floor room whitewash custom prison ...</td>\n",
       "      <td>alexandre dumas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1898</th>\n",
       "      <td>time littl woman ad rub head signific settl ye...</td>\n",
       "      <td>charles dickens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2662</th>\n",
       "      <td>deal amount vehem make mind speak final finish...</td>\n",
       "      <td>charles dickens</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1183 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              text_chunk              author\n",
       "5881   deceiv play joke excel read ah true said mont ...     alexandre dumas\n",
       "882    heel head wish wos still say prewar sir let bo...     charles dickens\n",
       "16471  would choos num atho artagnan said noth silenc...     alexandre dumas\n",
       "3294   littl earlier morn keep account attend houseke...     charles dickens\n",
       "13959  shall get back upon lackey hors _pardieu_ anyb...     alexandre dumas\n",
       "...                                                  ...                 ...\n",
       "8226   heart long anoth father polya papa fear angri ...  fyodor dostoyevsky\n",
       "11409  note often grow paler take princ took note fer...  fyodor dostoyevsky\n",
       "7376   room first floor room whitewash custom prison ...     alexandre dumas\n",
       "1898   time littl woman ad rub head signific settl ye...     charles dickens\n",
       "2662   deal amount vehem make mind speak final finish...     charles dickens\n",
       "\n",
       "[1183 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_chunk</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12339</th>\n",
       "      <td>quit sure reach culmin point happi num day saw...</td>\n",
       "      <td>fyodor dostoyevsky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4185</th>\n",
       "      <td>spain itali mercédè father could join fear liv...</td>\n",
       "      <td>alexandre dumas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15378</th>\n",
       "      <td>found pale fatigu inquir whether ill fact said...</td>\n",
       "      <td>alexandre dumas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9926</th>\n",
       "      <td>empti comfort said mrs corney much inde said b...</td>\n",
       "      <td>charles dickens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6920</th>\n",
       "      <td>crush singl touch word breath yes self thought...</td>\n",
       "      <td>alexandre dumas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2448</th>\n",
       "      <td>poor dear girl found much admir good disposit ...</td>\n",
       "      <td>charles dickens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14473</th>\n",
       "      <td>dispos convers reclin corner carriag num pass ...</td>\n",
       "      <td>alexandre dumas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14223</th>\n",
       "      <td>smile indic knew stori well wish relat recomme...</td>\n",
       "      <td>alexandre dumas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16470</th>\n",
       "      <td>shall begin portho arami drew back disappoint ...</td>\n",
       "      <td>alexandre dumas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>slowli shook shadow turn shall go home father ...</td>\n",
       "      <td>charles dickens</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5069 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              text_chunk              author\n",
       "12339  quit sure reach culmin point happi num day saw...  fyodor dostoyevsky\n",
       "4185   spain itali mercédè father could join fear liv...     alexandre dumas\n",
       "15378  found pale fatigu inquir whether ill fact said...     alexandre dumas\n",
       "9926   empti comfort said mrs corney much inde said b...     charles dickens\n",
       "6920   crush singl touch word breath yes self thought...     alexandre dumas\n",
       "...                                                  ...                 ...\n",
       "2448   poor dear girl found much admir good disposit ...     charles dickens\n",
       "14473  dispos convers reclin corner carriag num pass ...     alexandre dumas\n",
       "14223  smile indic knew stori well wish relat recomme...     alexandre dumas\n",
       "16470  shall begin portho arami drew back disappoint ...     alexandre dumas\n",
       "223    slowli shook shadow turn shall go home father ...     charles dickens\n",
       "\n",
       "[5069 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def summary_by_author(train_df, validation_df, test_df):\n",
    "    \"\"\"\n",
    "    Generates a summary table showing the number of samples per author for the training, validation, and testing sets.\n",
    "    \n",
    "    Args:\n",
    "        train_df (pd.DataFrame): Training DataFrame.\n",
    "        validation_df (pd.DataFrame): Validation DataFrame.\n",
    "        test_df (pd.DataFrame): Testing DataFrame.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: A summary DataFrame.\n",
    "    \"\"\"\n",
    "    \n",
    "    summary_data = {\n",
    "        'Author': train_df['author'].unique(),\n",
    "        'Train': train_df['author'].value_counts(),\n",
    "        'Validation': validation_df['author'].value_counts(),\n",
    "        'Test': test_df['author'].value_counts()\n",
    "    }\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    summary_df = summary_df.fillna(0)  # Replace NaN with 0 if no samples exist for some authors\n",
    "    \n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Train</th>\n",
       "      <th>Validation</th>\n",
       "      <th>Test</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>author</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>alexandre dumas</th>\n",
       "      <td>charles dickens</td>\n",
       "      <td>4744</td>\n",
       "      <td>527</td>\n",
       "      <td>2260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>charles dickens</th>\n",
       "      <td>alexandre dumas</td>\n",
       "      <td>3307</td>\n",
       "      <td>368</td>\n",
       "      <td>1575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fyodor dostoyevsky</th>\n",
       "      <td>fyodor dostoyevsky</td>\n",
       "      <td>2592</td>\n",
       "      <td>288</td>\n",
       "      <td>1234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Author  Train  Validation  Test\n",
       "author                                                         \n",
       "alexandre dumas        charles dickens   4744         527  2260\n",
       "charles dickens        alexandre dumas   3307         368  1575\n",
       "fyodor dostoyevsky  fyodor dostoyevsky   2592         288  1234"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_by_author(train_df, val_df, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feed Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 27219, Embedding Dimension: 1000\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Cargar el modelo Word2Vec\n",
    "word2vec_model = Word2Vec.load('data/answers/Books_1000_6.model')\n",
    "\n",
    "# Obtener la matriz de embeddings\n",
    "embedding_matrix = word2vec_model.wv.vectors  # Matriz de vectores\n",
    "\n",
    "# Tamaño del vocabulario y dimensión de los embeddings\n",
    "vocab_size = embedding_matrix.shape[0]\n",
    "embedding_dim = embedding_matrix.shape[1]\n",
    "\n",
    "print(f\"Vocab Size: {vocab_size}, Embedding Dimension: {embedding_dim}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Crear el diccionario de palabras a índices basado en el modelo Word2Vec\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.word_index = {word: idx for idx, word in enumerate(word2vec_model.wv.index_to_key)}\n",
    "\n",
    "# Convertir los textos en secuencias de índices\n",
    "train_sequences = tokenizer.texts_to_sequences(train_df['text_chunk'].tolist())\n",
    "val_sequences = tokenizer.texts_to_sequences(val_df['text_chunk'].tolist())\n",
    "test_sequences = tokenizer.texts_to_sequences(test_df['text_chunk'].tolist())\n",
    "\n",
    "# Rellenar las secuencias para que todas tengan la misma longitud\n",
    "maxlen = 200 \n",
    "train_sequences_padded = pad_sequences(train_sequences, maxlen=maxlen, padding='post')\n",
    "val_sequences_padded = pad_sequences(val_sequences, maxlen=maxlen, padding='post')\n",
    "test_sequences_padded = pad_sequences(test_sequences, maxlen=maxlen, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Inicializar el codificador\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Codificar las etiquetas de los autores como enteros\n",
    "train_labels_encoded = label_encoder.fit_transform(train_df['author'])\n",
    "val_labels_encoded = label_encoder.transform(val_df['author'])\n",
    "test_labels_encoded = label_encoder.transform(test_df['author'])\n",
    "\n",
    "# Convertir a formato de una-hot (one-hot encoding)\n",
    "train_labels_onehot = to_categorical(train_labels_encoded)\n",
    "val_labels_onehot = to_categorical(val_labels_encoded)\n",
    "test_labels_onehot = to_categorical(test_labels_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, Flatten, Dropout\n",
    "\n",
    "def model_1(vocab_size, embedding_dim, embedding_matrix, maxlen):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, \n",
    "                        weights=[embedding_matrix], input_length=maxlen, trainable=False))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(3, activation='softmax'))  # Tres clases\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def model_2(vocab_size, embedding_dim, embedding_matrix, maxlen):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, \n",
    "                        weights=[embedding_matrix], input_length=maxlen, trainable=False))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def model_3(vocab_size, embedding_dim, embedding_matrix, maxlen):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, \n",
    "                        weights=[embedding_matrix], input_length=maxlen, trainable=False))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Definir EarlyStopping para que detenga el entrenamiento si no hay mejora en 5 épocas\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USUARIO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 51ms/step - accuracy: 0.4268 - loss: 1.7509 - val_accuracy: 0.4455 - val_loss: 1.0726\n",
      "Epoch 2/1000\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 50ms/step - accuracy: 0.4445 - loss: 1.0716 - val_accuracy: 0.4455 - val_loss: 1.0679\n",
      "Epoch 3/1000\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 49ms/step - accuracy: 0.4430 - loss: 1.0674 - val_accuracy: 0.4455 - val_loss: 1.0675\n",
      "Epoch 4/1000\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 46ms/step - accuracy: 0.4402 - loss: 1.0714 - val_accuracy: 0.4455 - val_loss: 1.0674\n",
      "Epoch 5/1000\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 52ms/step - accuracy: 0.4465 - loss: 1.0676 - val_accuracy: 0.4455 - val_loss: 1.0674\n",
      "Epoch 6/1000\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 50ms/step - accuracy: 0.4544 - loss: 1.0622 - val_accuracy: 0.4455 - val_loss: 1.0674\n",
      "Epoch 7/1000\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 49ms/step - accuracy: 0.4556 - loss: 1.0629 - val_accuracy: 0.4455 - val_loss: 1.0674\n",
      "Epoch 8/1000\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 49ms/step - accuracy: 0.4442 - loss: 1.0673 - val_accuracy: 0.4455 - val_loss: 1.0674\n",
      "Epoch 9/1000\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 48ms/step - accuracy: 0.4460 - loss: 1.0679 - val_accuracy: 0.4455 - val_loss: 1.0674\n",
      "Epoch 10/1000\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 52ms/step - accuracy: 0.4430 - loss: 1.0691 - val_accuracy: 0.4455 - val_loss: 1.0674\n",
      "Epoch 11/1000\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 50ms/step - accuracy: 0.4501 - loss: 1.0653 - val_accuracy: 0.4455 - val_loss: 1.0674\n",
      "Epoch 12/1000\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 49ms/step - accuracy: 0.4421 - loss: 1.0695 - val_accuracy: 0.4455 - val_loss: 1.0674\n",
      "Epoch 13/1000\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 49ms/step - accuracy: 0.4355 - loss: 1.0714 - val_accuracy: 0.4455 - val_loss: 1.0674\n",
      "Epoch 14/1000\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 53ms/step - accuracy: 0.4426 - loss: 1.0682 - val_accuracy: 0.4455 - val_loss: 1.0674\n",
      "Epoch 15/1000\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 51ms/step - accuracy: 0.4423 - loss: 1.0693 - val_accuracy: 0.4455 - val_loss: 1.0674\n",
      "Epoch 16/1000\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 48ms/step - accuracy: 0.4434 - loss: 1.0682 - val_accuracy: 0.4455 - val_loss: 1.0674\n",
      "Epoch 17/1000\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 48ms/step - accuracy: 0.4497 - loss: 1.0650 - val_accuracy: 0.4455 - val_loss: 1.0674\n",
      "Epoch 18/1000\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 52ms/step - accuracy: 0.4459 - loss: 1.0669 - val_accuracy: 0.4455 - val_loss: 1.0674\n",
      "Epoch 19/1000\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 49ms/step - accuracy: 0.4400 - loss: 1.0707 - val_accuracy: 0.4455 - val_loss: 1.0674\n",
      "Epoch 20/1000\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 49ms/step - accuracy: 0.4444 - loss: 1.0693 - val_accuracy: 0.4455 - val_loss: 1.0674\n",
      "Epoch 21/1000\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 47ms/step - accuracy: 0.4480 - loss: 1.0656 - val_accuracy: 0.4455 - val_loss: 1.0674\n",
      "Epoch 22/1000\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 50ms/step - accuracy: 0.4476 - loss: 1.0662 - val_accuracy: 0.4455 - val_loss: 1.0674\n",
      "Epoch 23/1000\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 51ms/step - accuracy: 0.4499 - loss: 1.0666 - val_accuracy: 0.4455 - val_loss: 1.0674\n",
      "Epoch 24/1000\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 52ms/step - accuracy: 0.4439 - loss: 1.0690 - val_accuracy: 0.4455 - val_loss: 1.0674\n",
      "Epoch 25/1000\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 49ms/step - accuracy: 0.4393 - loss: 1.0711 - val_accuracy: 0.4455 - val_loss: 1.0674\n",
      "Epoch 26/1000\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 52ms/step - accuracy: 0.4524 - loss: 1.0642 - val_accuracy: 0.4455 - val_loss: 1.0674\n",
      "Epoch 27/1000\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 49ms/step - accuracy: 0.4553 - loss: 1.0641 - val_accuracy: 0.4455 - val_loss: 1.0675\n",
      "Epoch 28/1000\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 51ms/step - accuracy: 0.4525 - loss: 1.0648 - val_accuracy: 0.4455 - val_loss: 1.0674\n",
      "Epoch 29/1000\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 51ms/step - accuracy: 0.4438 - loss: 1.0680 - val_accuracy: 0.4455 - val_loss: 1.0674\n",
      "Epoch 30/1000\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 52ms/step - accuracy: 0.4508 - loss: 1.0661 - val_accuracy: 0.4455 - val_loss: 1.0674\n",
      "Epoch 31/1000\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 52ms/step - accuracy: 0.4499 - loss: 1.0669 - val_accuracy: 0.4455 - val_loss: 1.0674\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1c6614e5d10>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrenar la primera red\n",
    "model1 = model_1(vocab_size, embedding_dim, embedding_matrix, maxlen)\n",
    "model1.fit(train_sequences_padded, train_labels_onehot, \n",
    "           validation_data=(val_sequences_padded, val_labels_onehot), epochs=1000, callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 89ms/step - accuracy: 0.4277 - loss: 2.1973 - val_accuracy: 0.4455 - val_loss: 1.0690\n",
      "Epoch 2/1000\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 87ms/step - accuracy: 0.4500 - loss: 1.0667 - val_accuracy: 0.4455 - val_loss: 1.0677\n",
      "Epoch 3/1000\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 85ms/step - accuracy: 0.4415 - loss: 1.0699 - val_accuracy: 0.4455 - val_loss: 1.0674\n",
      "Epoch 4/1000\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 88ms/step - accuracy: 0.4366 - loss: 1.0709 - val_accuracy: 0.4455 - val_loss: 1.0675\n",
      "Epoch 5/1000\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 86ms/step - accuracy: 0.4427 - loss: 1.0690 - val_accuracy: 0.4455 - val_loss: 1.0674\n",
      "Epoch 6/1000\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 88ms/step - accuracy: 0.4462 - loss: 1.0678 - val_accuracy: 0.4455 - val_loss: 1.0674\n",
      "Epoch 7/1000\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 87ms/step - accuracy: 0.4392 - loss: 1.0706 - val_accuracy: 0.4455 - val_loss: 1.0674\n",
      "Epoch 8/1000\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 88ms/step - accuracy: 0.4453 - loss: 1.0677 - val_accuracy: 0.4455 - val_loss: 1.0674\n",
      "Epoch 9/1000\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 90ms/step - accuracy: 0.4440 - loss: 1.0662 - val_accuracy: 0.4455 - val_loss: 1.0675\n",
      "Epoch 10/1000\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 86ms/step - accuracy: 0.4403 - loss: 1.0709 - val_accuracy: 0.4455 - val_loss: 1.0674\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1c664f3f850>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrenar la segunda red\n",
    "model2 = model_2(vocab_size, embedding_dim, embedding_matrix, maxlen)\n",
    "model2.fit(train_sequences_padded, train_labels_onehot, \n",
    "           validation_data=(val_sequences_padded, val_labels_onehot), epochs=1000, callbacks=[early_stopping])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 162ms/step - accuracy: 0.4060 - loss: 4.1586 - val_accuracy: 0.4455 - val_loss: 1.0682\n",
      "Epoch 2/1000\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 170ms/step - accuracy: 0.4449 - loss: 1.0749 - val_accuracy: 0.4455 - val_loss: 1.0675\n",
      "Epoch 3/1000\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 171ms/step - accuracy: 0.4462 - loss: 1.0693 - val_accuracy: 0.4455 - val_loss: 1.0676\n",
      "Epoch 4/1000\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 161ms/step - accuracy: 0.4508 - loss: 1.0656 - val_accuracy: 0.4455 - val_loss: 1.0675\n",
      "Epoch 5/1000\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 160ms/step - accuracy: 0.4449 - loss: 1.0684 - val_accuracy: 0.4455 - val_loss: 1.0675\n",
      "Epoch 6/1000\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 159ms/step - accuracy: 0.4419 - loss: 1.0699 - val_accuracy: 0.4455 - val_loss: 1.0674\n",
      "Epoch 7/1000\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 153ms/step - accuracy: 0.4397 - loss: 1.0713 - val_accuracy: 0.4455 - val_loss: 1.0674\n",
      "Epoch 8/1000\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 156ms/step - accuracy: 0.4424 - loss: 1.0684 - val_accuracy: 0.4455 - val_loss: 1.0674\n",
      "Epoch 9/1000\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 159ms/step - accuracy: 0.4507 - loss: 1.0667 - val_accuracy: 0.4455 - val_loss: 1.0675\n",
      "Epoch 10/1000\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 154ms/step - accuracy: 0.4545 - loss: 1.0638 - val_accuracy: 0.4455 - val_loss: 1.0674\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1c665852950>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrenar la tercera red\n",
    "model3 = model_3(vocab_size, embedding_dim, embedding_matrix, maxlen)\n",
    "model3.fit(train_sequences_padded, train_labels_onehot, \n",
    "           validation_data=(val_sequences_padded, val_labels_onehot), epochs=1000, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metricas frente a test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n"
     ]
    }
   ],
   "source": [
    "# Obtener las predicciones del modelo sobre el conjunto de prueba\n",
    "predictions = model1.predict(test_sequences_padded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convertir las probabilidades a etiquetas (seleccionando la clase con la mayor probabilidad)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.44584730716117577\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      1.00      0.62      2260\n",
      "           1       0.00      0.00      0.00      1575\n",
      "           2       0.00      0.00      0.00      1234\n",
      "\n",
      "    accuracy                           0.45      5069\n",
      "   macro avg       0.15      0.33      0.21      5069\n",
      "weighted avg       0.20      0.45      0.27      5069\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2260    0    0]\n",
      " [1575    0    0]\n",
      " [1234    0    0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USUARIO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\USUARIO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\USUARIO\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Calcular la precisión (accuracy)\n",
    "accuracy = accuracy_score(test_labels_encoded, predicted_classes)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Imprimir un informe de clasificación detallado\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(test_labels_encoded, predicted_classes))\n",
    "\n",
    "# Imprimir la matriz de confusión\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(test_labels_encoded, predicted_classes))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
