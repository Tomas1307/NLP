{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset creation for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow\n",
    "#!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from utils_processor.processor import Processor\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from typing import Tuple, List, Any\n",
    "from gensim.models import Word2Vec\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Flatten, Dropout, LSTM, GlobalMaxPooling1D\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "import numpy as np\n",
    "from typing import Tuple, Any\n",
    "from tensorflow.keras.layers import (\n",
    "        Input, Bidirectional, LayerNormalization, SpatialDropout1D,\n",
    "        BatchNormalization, Add, MultiHeadAttention, \n",
    "        Concatenate, GlobalAveragePooling1D\n",
    "    )\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor_ = Processor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando archivo: ATaleofTwoCities_Dickens.txt, lineas:  776878\n",
      "Procesando archivo: BleakHouse_Dickens.txt, lineas:  1958792\n",
      "Procesando archivo: CountofMonteCristo_Dumas.txt, lineas:  2646641\n",
      "Procesando archivo: CrimeAndPunishment_dostoyevski.txt, lineas:  1154409\n",
      "Procesando archivo: OliverTwist_Dickens.txt, lineas:  912421\n",
      "Procesando archivo: TheGambler_dostoyevski.txt, lineas:  350954\n",
      "Procesando archivo: TheIdiot_dostoyevski.txt, lineas:  1366983\n",
      "Procesando archivo: TheThreeMusketeers_Dumas.txt, lineas:  1317339\n",
      "Procesando archivo: TwentyYearsAfter_Dumas.txt, lineas:  1387344\n",
      "Lista de autores en orden:\n",
      "1. charles dickens\n",
      "2. charles dickens\n",
      "3. alexandre dumas\n",
      "4. fyodor dostoyevsky\n",
      "5. charles dickens\n",
      "6. fyodor dostoyevsky\n",
      "7. fyodor dostoyevsky\n",
      "8. alexandre dumas\n",
      "9. alexandre dumas\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['charles dickens',\n",
       " 'charles dickens',\n",
       " 'alexandre dumas',\n",
       " 'fyodor dostoyevsky',\n",
       " 'charles dickens',\n",
       " 'fyodor dostoyevsky',\n",
       " 'fyodor dostoyevsky',\n",
       " 'alexandre dumas',\n",
       " 'alexandre dumas']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "# Directorio donde están los archivos .txt\n",
    "data_dir = 'data/books/'\n",
    "\n",
    "# Lista para almacenar los textos y autores\n",
    "texts = []\n",
    "authors = []\n",
    "\n",
    "# Leer todos los archivos .txt del directorio\n",
    "for filename in os.listdir(data_dir):\n",
    "    if filename.endswith('.txt'):\n",
    "\n",
    "        with open(os.path.join(data_dir, filename), 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "            texts.append(text)  # Almacenar el texto\n",
    "            print(f\"Procesando archivo: {filename}, lineas: \", len(text))\n",
    "            \n",
    "            # Buscar el nombre del autor\n",
    "            author_match = re.search(r'Author:\\s*(.+)', text)\n",
    "            if author_match:\n",
    "                author_name = author_match.group(1).strip()\n",
    "                authors.append(author_name.lower())  # Almacenar el autor\n",
    "            else:\n",
    "                authors.append(\"Autor no encontrado\")  # En caso de no encontrarlo\n",
    "            \n",
    "            # Mostrar un fragmento del texto (opcional)\n",
    "            #print(text[:2500])\n",
    "\n",
    "# Mostrar los autores encontrados\n",
    "print(\"Lista de autores en orden:\")\n",
    "for i, author in enumerate(authors):\n",
    "    print(f\"{i+1}. {author}\")\n",
    "\n",
    "# Ahora tienes dos listas: 'texts' con los textos y 'authors' con los autores en el mismo orden\n",
    "authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_texts(processor = Processor(), texts: list = []):\n",
    "    \"\"\"\n",
    "    Processes a list of texts and logs progress for each one, using the Processor class.\n",
    "    \n",
    "    Args:\n",
    "        processor (Processor): An instance of the Processor class.\n",
    "        texts (list): A list of text strings to process.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of processed texts.\n",
    "    \"\"\"\n",
    "    total = len(texts)\n",
    "    processed_texts = []\n",
    "    \n",
    "    for index, text in enumerate(texts):\n",
    "        processed_text = processor.preprocessing_pipeline_as_chunks(text, index, total)\n",
    "        processed_texts.append(processed_text)  # Guardamos el texto procesado como lista de tokens\n",
    "    \n",
    "    return processed_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procesar todos los textos con el sistema de logging\n",
    "processed_texts = process_all_texts(processor_, texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks = []\n",
    "chunk_authors = []\n",
    "\n",
    "for i, text_list in enumerate(processed_texts):\n",
    "    author = authors[i]\n",
    "    for chunk in text_list:\n",
    "        text_chunks.append(chunk)  # Agregar cada chunk de texto\n",
    "        chunk_authors.append(author)  # Agregar el autor correspondiente\n",
    "\n",
    "# Crear un DataFrame con las listas\n",
    "df_chunks = pd.DataFrame({\n",
    "    'text_chunk': text_chunks,\n",
    "    'author': chunk_authors\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_chunk</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tale num citi tale num citi stori french revol...</td>\n",
       "      <td>charles dickens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chapter xv knit chapter xvi still knit chapter...</td>\n",
       "      <td>charles dickens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>life chapter period best time worst time age w...</td>\n",
       "      <td>charles dickens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>king larg jaw queen plain face throne england ...</td>\n",
       "      <td>charles dickens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>london westminst even cocklan ghost laid round...</td>\n",
       "      <td>charles dickens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16890</th>\n",
       "      <td>critic reach project gutenberg goal ensur proj...</td>\n",
       "      <td>alexandre dumas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16891</th>\n",
       "      <td>num contribut project gutenberg literari archi...</td>\n",
       "      <td>alexandre dumas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16892</th>\n",
       "      <td>num num particular import maintain tax exempt ...</td>\n",
       "      <td>alexandre dumas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16893</th>\n",
       "      <td>us offer donat intern donat grate accept make ...</td>\n",
       "      <td>alexandre dumas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16894</th>\n",
       "      <td>often creat sever print edit confirm protect c...</td>\n",
       "      <td>alexandre dumas</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16895 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              text_chunk           author\n",
       "0      tale num citi tale num citi stori french revol...  charles dickens\n",
       "1      chapter xv knit chapter xvi still knit chapter...  charles dickens\n",
       "2      life chapter period best time worst time age w...  charles dickens\n",
       "3      king larg jaw queen plain face throne england ...  charles dickens\n",
       "4      london westminst even cocklan ghost laid round...  charles dickens\n",
       "...                                                  ...              ...\n",
       "16890  critic reach project gutenberg goal ensur proj...  alexandre dumas\n",
       "16891  num contribut project gutenberg literari archi...  alexandre dumas\n",
       "16892  num num particular import maintain tax exempt ...  alexandre dumas\n",
       "16893  us offer donat intern donat grate accept make ...  alexandre dumas\n",
       "16894  often creat sever print edit confirm protect c...  alexandre dumas\n",
       "\n",
       "[16895 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Prepare the text data\n",
    "\n",
    "We already have the processed texts stored in a list called processed_texts. Each element in this list represents the chunks of text (after splitting) for a particular book.\n",
    "Each entry in processed_texts is a list where each element is a chunk of text for that book, processed based on the method we implemented for splitting into chunks of 150 words with a 25-word overlap\n",
    "\n",
    "2. Prepare the author labels\n",
    "\n",
    "We have an authors list that stores the corresponding author for each book in processed_texts. Each author appears multiple times if they have multiple books in the dataset. \n",
    "\n",
    "3. Create the DataFrame structure\n",
    "\n",
    "For each processed book (i.e., processed_texts[i]), we know that all the chunks of that book correspond to a specific author. So we can assign the same author to all the chunks in that list.\n",
    "We will loop over each entry in processed_texts and for each chunk, add it to a DataFrame, along with the corresponding author.\n",
    "\n",
    "4. Steps to build the DataFrame\n",
    "\n",
    "* Initialize lists for the DataFrame: We will initialize two lists: one for text chunks and one for authors.\n",
    "* Iterate over processed_texts: For each entry in processed_texts, we extract the list of chunks and the corresponding author.\n",
    "* Add chunks and authors to the lists: For each chunk in the list of text chunks, we append it to the \"text_chunk\" list and the corresponding author to the \"author\" list.\n",
    "* Create the DataFrame: Once the lists are filled, we create a pandas DataFrame with two columns: \"text_chunk\" and \"author\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming df_chunks is the dataframe with the columns ['text_chunk', 'author']\n",
    "\n",
    "# Step 1: Split the dataset into 70% training and 30% test\n",
    "train_df, test_df = train_test_split(df_chunks, test_size=0.30, stratify=df_chunks['author'], random_state=42)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.1, stratify=train_df['author'], random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_chunk</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10344</th>\n",
       "      <td>elbow beg pardon said dodger look air abstract...</td>\n",
       "      <td>charles dickens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2552</th>\n",
       "      <td>pursu say emphat william guppi drop mr guppi a...</td>\n",
       "      <td>charles dickens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13609</th>\n",
       "      <td>secret use know anyth said young woman instinc...</td>\n",
       "      <td>alexandre dumas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3369</th>\n",
       "      <td>look keep secret condescens present visit feel...</td>\n",
       "      <td>charles dickens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9896</th>\n",
       "      <td>jew sooner alon counten resum former express a...</td>\n",
       "      <td>charles dickens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5799</th>\n",
       "      <td>better inform know owner hors shut cri pit cho...</td>\n",
       "      <td>alexandre dumas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8183</th>\n",
       "      <td>began count third excus would say fanci made m...</td>\n",
       "      <td>fyodor dostoyevsky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13251</th>\n",
       "      <td>take away commiss give mademoisell de chemerau...</td>\n",
       "      <td>alexandre dumas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4851</th>\n",
       "      <td>must curios natur island mass rock contain acr...</td>\n",
       "      <td>alexandre dumas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9704</th>\n",
       "      <td>harden littl wretch take away said mr bumbl im...</td>\n",
       "      <td>charles dickens</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10643 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              text_chunk              author\n",
       "10344  elbow beg pardon said dodger look air abstract...     charles dickens\n",
       "2552   pursu say emphat william guppi drop mr guppi a...     charles dickens\n",
       "13609  secret use know anyth said young woman instinc...     alexandre dumas\n",
       "3369   look keep secret condescens present visit feel...     charles dickens\n",
       "9896   jew sooner alon counten resum former express a...     charles dickens\n",
       "...                                                  ...                 ...\n",
       "5799   better inform know owner hors shut cri pit cho...     alexandre dumas\n",
       "8183   began count third excus would say fanci made m...  fyodor dostoyevsky\n",
       "13251  take away commiss give mademoisell de chemerau...     alexandre dumas\n",
       "4851   must curios natur island mass rock contain acr...     alexandre dumas\n",
       "9704   harden littl wretch take away said mr bumbl im...     charles dickens\n",
       "\n",
       "[10643 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_chunk</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5881</th>\n",
       "      <td>deceiv play joke excel read ah true said mont ...</td>\n",
       "      <td>alexandre dumas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>heel head wish wos still say prewar sir let bo...</td>\n",
       "      <td>charles dickens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16471</th>\n",
       "      <td>would choos num atho artagnan said noth silenc...</td>\n",
       "      <td>alexandre dumas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3294</th>\n",
       "      <td>littl earlier morn keep account attend houseke...</td>\n",
       "      <td>charles dickens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13959</th>\n",
       "      <td>shall get back upon lackey hors _pardieu_ anyb...</td>\n",
       "      <td>alexandre dumas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8226</th>\n",
       "      <td>heart long anoth father polya papa fear angri ...</td>\n",
       "      <td>fyodor dostoyevsky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11409</th>\n",
       "      <td>note often grow paler take princ took note fer...</td>\n",
       "      <td>fyodor dostoyevsky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7376</th>\n",
       "      <td>room first floor room whitewash custom prison ...</td>\n",
       "      <td>alexandre dumas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1898</th>\n",
       "      <td>time littl woman ad rub head signific settl ye...</td>\n",
       "      <td>charles dickens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2662</th>\n",
       "      <td>deal amount vehem make mind speak final finish...</td>\n",
       "      <td>charles dickens</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1183 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              text_chunk              author\n",
       "5881   deceiv play joke excel read ah true said mont ...     alexandre dumas\n",
       "882    heel head wish wos still say prewar sir let bo...     charles dickens\n",
       "16471  would choos num atho artagnan said noth silenc...     alexandre dumas\n",
       "3294   littl earlier morn keep account attend houseke...     charles dickens\n",
       "13959  shall get back upon lackey hors _pardieu_ anyb...     alexandre dumas\n",
       "...                                                  ...                 ...\n",
       "8226   heart long anoth father polya papa fear angri ...  fyodor dostoyevsky\n",
       "11409  note often grow paler take princ took note fer...  fyodor dostoyevsky\n",
       "7376   room first floor room whitewash custom prison ...     alexandre dumas\n",
       "1898   time littl woman ad rub head signific settl ye...     charles dickens\n",
       "2662   deal amount vehem make mind speak final finish...     charles dickens\n",
       "\n",
       "[1183 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_chunk</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12339</th>\n",
       "      <td>quit sure reach culmin point happi num day saw...</td>\n",
       "      <td>fyodor dostoyevsky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4185</th>\n",
       "      <td>spain itali mercédè father could join fear liv...</td>\n",
       "      <td>alexandre dumas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15378</th>\n",
       "      <td>found pale fatigu inquir whether ill fact said...</td>\n",
       "      <td>alexandre dumas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9926</th>\n",
       "      <td>empti comfort said mrs corney much inde said b...</td>\n",
       "      <td>charles dickens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6920</th>\n",
       "      <td>crush singl touch word breath yes self thought...</td>\n",
       "      <td>alexandre dumas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2448</th>\n",
       "      <td>poor dear girl found much admir good disposit ...</td>\n",
       "      <td>charles dickens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14473</th>\n",
       "      <td>dispos convers reclin corner carriag num pass ...</td>\n",
       "      <td>alexandre dumas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14223</th>\n",
       "      <td>smile indic knew stori well wish relat recomme...</td>\n",
       "      <td>alexandre dumas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16470</th>\n",
       "      <td>shall begin portho arami drew back disappoint ...</td>\n",
       "      <td>alexandre dumas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>slowli shook shadow turn shall go home father ...</td>\n",
       "      <td>charles dickens</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5069 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              text_chunk              author\n",
       "12339  quit sure reach culmin point happi num day saw...  fyodor dostoyevsky\n",
       "4185   spain itali mercédè father could join fear liv...     alexandre dumas\n",
       "15378  found pale fatigu inquir whether ill fact said...     alexandre dumas\n",
       "9926   empti comfort said mrs corney much inde said b...     charles dickens\n",
       "6920   crush singl touch word breath yes self thought...     alexandre dumas\n",
       "...                                                  ...                 ...\n",
       "2448   poor dear girl found much admir good disposit ...     charles dickens\n",
       "14473  dispos convers reclin corner carriag num pass ...     alexandre dumas\n",
       "14223  smile indic knew stori well wish relat recomme...     alexandre dumas\n",
       "16470  shall begin portho arami drew back disappoint ...     alexandre dumas\n",
       "223    slowli shook shadow turn shall go home father ...     charles dickens\n",
       "\n",
       "[5069 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def summary_by_author(train_df, validation_df, test_df):\n",
    "    \"\"\"\n",
    "    Generates a summary table showing the number of samples per author for the training, validation, and testing sets.\n",
    "    \n",
    "    Args:\n",
    "        train_df (pd.DataFrame): Training DataFrame.\n",
    "        validation_df (pd.DataFrame): Validation DataFrame.\n",
    "        test_df (pd.DataFrame): Testing DataFrame.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: A summary DataFrame.\n",
    "    \"\"\"\n",
    "    \n",
    "    summary_data = {\n",
    "        'Author': train_df['author'].unique(),\n",
    "        'Train': train_df['author'].value_counts(),\n",
    "        'Validation': validation_df['author'].value_counts(),\n",
    "        'Test': test_df['author'].value_counts()\n",
    "    }\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    summary_df = summary_df.fillna(0)  # Replace NaN with 0 if no samples exist for some authors\n",
    "    \n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Train</th>\n",
       "      <th>Validation</th>\n",
       "      <th>Test</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>author</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>alexandre dumas</th>\n",
       "      <td>charles dickens</td>\n",
       "      <td>4744</td>\n",
       "      <td>527</td>\n",
       "      <td>2260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>charles dickens</th>\n",
       "      <td>alexandre dumas</td>\n",
       "      <td>3307</td>\n",
       "      <td>368</td>\n",
       "      <td>1575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fyodor dostoyevsky</th>\n",
       "      <td>fyodor dostoyevsky</td>\n",
       "      <td>2592</td>\n",
       "      <td>288</td>\n",
       "      <td>1234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Author  Train  Validation  Test\n",
       "author                                                         \n",
       "alexandre dumas        charles dickens   4744         527  2260\n",
       "charles dickens        alexandre dumas   3307         368  1575\n",
       "fyodor dostoyevsky  fyodor dostoyevsky   2592         288  1234"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_by_author(train_df, val_df, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feed Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from gensim.models import Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df, processor_, word2vec_model):\n",
    "    X = []\n",
    "    print(\"Procesando textos...\")\n",
    "    \n",
    "    for text in df['text_chunk']:\n",
    "        processed_tokens = processor_.preprocessing_pipeline(text)\n",
    "        X.append(processed_tokens)\n",
    "    \n",
    "    try:\n",
    "        vocab = word2vec_model.wv.key_to_index\n",
    "    except AttributeError:\n",
    "        vocab = word2vec_model.key_to_index\n",
    "    X_indices = []\n",
    "    for tokens in X:\n",
    "        indices = []\n",
    "        for token in tokens:\n",
    "            indices.append(vocab.get(token, 0))  # 0 para tokens desconocidos\n",
    "        X_indices.append(indices)\n",
    "    \n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_embedding_matrix(train_df, val_df, test_df, word2vec_model: Word2Vec, embedding_size: int= 200):\n",
    "\n",
    "    X_train = prepare_data(train_df, processor_, word2vec_model=word2vec_model)\n",
    "    X_val = prepare_data(val_df, processor_, word2vec_model= word2vec_model)\n",
    "    X_test = prepare_data(test_df, processor_, word2vec_model=word2vec_model)\n",
    "\n",
    "    max_length = max(\n",
    "        max(len(x) for x in X_train),\n",
    "        max(len(x) for x in X_val),\n",
    "        max(len(x) for x in X_test)\n",
    "    )\n",
    "    print(f\"Longitud máxima de secuencia: {max_length}\")\n",
    "\n",
    "    X_train_padded = pad_sequences(X_train, maxlen=max_length, padding='post')\n",
    "    X_val_padded = pad_sequences(X_val, maxlen=max_length, padding='post')\n",
    "    X_test_padded = pad_sequences(X_test, maxlen=max_length, padding='post')\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(train_df['author'])\n",
    "\n",
    "    y_train = label_encoder.transform(train_df['author'])\n",
    "    y_val = label_encoder.transform(val_df['author'])\n",
    "    y_test = label_encoder.transform(test_df['author'])\n",
    "\n",
    "    try:\n",
    "        vocab = word2vec_model.wv.key_to_index  \n",
    "    except AttributeError:\n",
    "        print(\"Glove model detected\")\n",
    "        vocab = word2vec_model.key_to_index \n",
    "\n",
    "    vocab_size = len(vocab) + 1\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_size))\n",
    "        \n",
    "    for word, idx in vocab.items():\n",
    "        try:\n",
    "            embedding_matrix[idx] = word2vec_model.wv[word]\n",
    "        except AttributeError:\n",
    "            embedding_matrix[idx] = word2vec_model[word]\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RDD1(embedding_matrix: np.ndarray, vocab_size: int, embedding_size: int, max_length: int) -> Sequential:\n",
    "    \"\"\"\n",
    "    Creates a neural network architecture with multiple dense layers and dropout for author classification.\n",
    "    This model uses a pre-trained embedding layer followed by several dense layers with increasing complexity.\n",
    "\n",
    "    Architecture details:\n",
    "    - Non-trainable embedding layer using pre-trained weights\n",
    "    - Flatten layer to convert 3D tensor to 2D\n",
    "    - Three dense layers with increasing complexity\n",
    "    - Dropout layers to prevent overfitting\n",
    "    - Softmax output layer for classification\n",
    "\n",
    "    Args:\n",
    "        embedding_matrix (np.ndarray): Pre-trained embedding matrix for word vectors\n",
    "        vocab_size (int): Size of the vocabulary (number of unique words)\n",
    "        embedding_size (int): Dimension of the embedding vectors\n",
    "        max_length (int): Maximum length of input sequences\n",
    "\n",
    "    Returns:\n",
    "        Sequential: Compiled Keras model ready for training\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=vocab_size, \n",
    "                 output_dim=embedding_size, \n",
    "                 weights=[embedding_matrix], \n",
    "                 input_length=max_length, \n",
    "                 trainable=False),\n",
    "        Flatten(),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(3, activation='softmax')  # 3 classes for authors\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def RDD2(embedding_matrix: np.ndarray, vocab_size: int, embedding_size: int, max_length: int) -> Sequential:\n",
    "    \"\"\"\n",
    "    Creates an advanced neural network architecture combining LSTM and dense layers for author classification.\n",
    "    This model is designed to capture both sequential patterns and complex feature interactions.\n",
    "\n",
    "    Architecture details:\n",
    "    - Non-trainable embedding layer using pre-trained weights\n",
    "    - Bidirectional LSTM layer for sequence processing\n",
    "    - Global max pooling to reduce dimensionality\n",
    "    - Multiple dense layers with batch normalization\n",
    "    - Dropout layers for regularization\n",
    "    - Softmax output layer for classification\n",
    "\n",
    "    Args:\n",
    "        embedding_matrix (np.ndarray): Pre-trained embedding matrix for word vectors\n",
    "        vocab_size (int): Size of the vocabulary (number of unique words)\n",
    "        embedding_size (int): Dimension of the embedding vectors\n",
    "        max_length (int): Maximum length of input sequences\n",
    "\n",
    "    Returns:\n",
    "        Sequential: Compiled Keras model ready for training\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=vocab_size, \n",
    "                 output_dim=embedding_size, \n",
    "                 weights=[embedding_matrix], \n",
    "                 input_length=max_length, \n",
    "                 trainable=False),\n",
    "        Flatten(),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dropout(0.4),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.4),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(3, activation='softmax')  # 3 classes for authors\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def RDD3(embedding_matrix: np.ndarray, vocab_size: int, embedding_size: int, max_length: int) -> Model:\n",
    "    \"\"\"\n",
    "    Creates an advanced neural network architecture combining Bidirectional LSTM layers,\n",
    "    residual connections, and attention mechanisms for sophisticated text processing.\n",
    "    This model is designed for complex natural language understanding tasks.\n",
    "\n",
    "    Architecture details:\n",
    "    - Non-trainable embedding layer using pre-trained weights\n",
    "    - Multiple Bidirectional LSTM layers with residual connections\n",
    "    - Multi-head attention layer for capturing long-range dependencies\n",
    "    - Advanced regularization techniques:\n",
    "        * Spatial Dropout for embeddings\n",
    "        * Layer normalization\n",
    "        * Gradient clipping\n",
    "        * Multiple dropout layers with varying rates\n",
    "    - Batch normalization for stable training\n",
    "    - Skip connections between layers\n",
    "    \n",
    "    Key features:\n",
    "    - Deep architecture with 3 Bidirectional LSTM layers\n",
    "    - Attention mechanism for better context understanding\n",
    "    - Residual connections to prevent vanishing gradients\n",
    "    - Multiple regularization techniques to prevent overfitting\n",
    "    - Progressive dimension reduction through the network\n",
    "\n",
    "    Args:\n",
    "        embedding_matrix (np.ndarray): Pre-trained embedding matrix for word vectors\n",
    "        vocab_size (int): Size of the vocabulary (number of unique words)\n",
    "        embedding_size (int): Dimension of the embedding vectors\n",
    "        max_length (int): Maximum length of input sequences\n",
    "\n",
    "    Returns:\n",
    "        Model: Compiled Keras model ready for training\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # Input layer\n",
    "    inputs = Input(shape=(max_length,))\n",
    "    \n",
    "    # Embedding layer with spatial dropout\n",
    "    x = Embedding(input_dim=vocab_size,\n",
    "                 output_dim=embedding_size,\n",
    "                 weights=[embedding_matrix],\n",
    "                 trainable=False)(inputs)\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    \n",
    "    # First Bidirectional LSTM with residual connection\n",
    "    lstm_out = 256\n",
    "    main_output = Bidirectional(LSTM(lstm_out, \n",
    "                                   return_sequences=True,\n",
    "                                   dropout=0.2,\n",
    "                                   recurrent_dropout=0.2))(x)\n",
    "    main_output = LayerNormalization()(main_output)\n",
    "    \n",
    "    # Multi-head attention layer\n",
    "    attention_output = MultiHeadAttention(\n",
    "        num_heads=8, \n",
    "        key_dim=32\n",
    "    )(main_output, main_output)\n",
    "    \n",
    "    # Add & Normalize (residual connection)\n",
    "    main_output = Add()([main_output, attention_output])\n",
    "    main_output = LayerNormalization()(main_output)\n",
    "    \n",
    "    # Second Bidirectional LSTM\n",
    "    lstm_2 = Bidirectional(LSTM(lstm_out//2,\n",
    "                               return_sequences=True,\n",
    "                               dropout=0.2,\n",
    "                               recurrent_dropout=0.2))(main_output)\n",
    "    lstm_2 = LayerNormalization()(lstm_2)\n",
    "    \n",
    "    # Third Bidirectional LSTM\n",
    "    lstm_3 = Bidirectional(LSTM(lstm_out//4,\n",
    "                               return_sequences=True,\n",
    "                               dropout=0.2,\n",
    "                               recurrent_dropout=0.2))(lstm_2)\n",
    "    lstm_3 = LayerNormalization()(lstm_3)\n",
    "    \n",
    "    # Combine different levels of features\n",
    "    pooled_outputs = [\n",
    "        GlobalAveragePooling1D()(main_output),\n",
    "        GlobalAveragePooling1D()(lstm_2),\n",
    "        GlobalAveragePooling1D()(lstm_3)\n",
    "    ]\n",
    "    concat = Concatenate()(pooled_outputs)\n",
    "    \n",
    "    # Dense layers with batch normalization and dropout\n",
    "    x = Dense(512, activation='relu')(concat)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    \n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    # Output layer\n",
    "    outputs = Dense(3, activation='softmax')(x)\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_model(model: Sequential, \n",
    "                train_data: Tuple[np.ndarray, np.ndarray],\n",
    "                val_data: Tuple[np.ndarray, np.ndarray],\n",
    "                batch_size: int = 32,\n",
    "                epochs: int = 20) -> Any:\n",
    "    \"\"\"\n",
    "    Trains the neural network model with early stopping and learning rate scheduling.\n",
    "    Implements best practices for model training including early stopping and \n",
    "    performance monitoring.\n",
    "\n",
    "    Training features:\n",
    "    - Early stopping to prevent overfitting\n",
    "    - Sparse categorical crossentropy loss for multi-class classification\n",
    "    - Adam optimizer with default learning rate\n",
    "    - Batch training with customizable batch size\n",
    "    - Validation monitoring during training\n",
    "\n",
    "    Args:\n",
    "        model (Sequential): The neural network model to train\n",
    "        train_data (Tuple[np.ndarray, np.ndarray]): Training data and labels\n",
    "        val_data (Tuple[np.ndarray, np.ndarray]): Validation data and labels\n",
    "        batch_size (int): Number of samples per gradient update (default: 32)\n",
    "        epochs (int): Maximum number of training epochs (default: 20)\n",
    "\n",
    "    Returns:\n",
    "        Any: Training history containing loss and metric values\n",
    "    \"\"\"\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=3,\n",
    "        restore_best_weights=True,\n",
    "        min_delta=0.001\n",
    "    )\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        train_data[0],\n",
    "        train_data[1],\n",
    "        validation_data=val_data,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return history\n",
    "\n",
    "def evaluate_model(model: Sequential, \n",
    "                  test_data: Tuple[np.ndarray, np.ndarray]) -> Tuple[float, float, float]:\n",
    "    \"\"\"\n",
    "    Evaluates the trained model using multiple performance metrics.\n",
    "    Provides a comprehensive evaluation of the model's performance on test data.\n",
    "\n",
    "    Evaluation metrics:\n",
    "    - Accuracy: Overall classification accuracy\n",
    "    - Precision: Weighted precision across all classes\n",
    "    - Recall: Weighted recall across all classes\n",
    "\n",
    "    Args:\n",
    "        model (Sequential): The trained neural network model\n",
    "        test_data (Tuple[np.ndarray, np.ndarray]): Test data and ground truth labels\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float, float]: Tuple containing accuracy, precision, and recall scores\n",
    "    \"\"\"\n",
    "    X_test, y_test = test_data\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred_classes)\n",
    "    precision = precision_score(y_test, y_pred_classes, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred_classes, average='weighted')\n",
    "    \n",
    "    return accuracy, precision, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glove embedding - 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_classification_pipeline_glove() -> None:\n",
    "    \"\"\"\n",
    "    Runs the complete classification pipeline for GloVe embeddings with different dimensionalities (50, 100, 200).\n",
    "    \"\"\"\n",
    "    print(\"=== Starting Author Classification Pipeline with GloVe ===\")\n",
    "    \n",
    "    try:\n",
    "        # Cargar embeddings preentrenados de GloVe\n",
    "        glove_models = {\n",
    "            #'glove_50': api.load('glove-wiki-gigaword-50'),   # 50 dimensiones\n",
    "            #'glove_100': api.load('glove-wiki-gigaword-100'),  # 100 dimensiones\n",
    "            'glove_200': api.load('glove-wiki-gigaword-200'),  # 200 dimensiones\n",
    "        }\n",
    "\n",
    "        print(\"\\n1. Processing text data...\")\n",
    "\n",
    "        results = {}\n",
    "\n",
    "        # Iterar sobre los modelos de GloVe de diferentes dimensiones\n",
    "        for glove_name, glove_model in glove_models.items():\n",
    "            embedding_size = int(glove_name.split('_')[1])  # Extraer el tamaño de los embeddings (50, 100, 200)\n",
    "            \n",
    "            print(f\"\\nPreparing embedding matrix and data for {glove_name}...\")\n",
    "\n",
    "            # Crear la matriz de embeddings para GloVe\n",
    "            embedding_matrix = create_embedding_matrix(\n",
    "                train_df, \n",
    "                val_df, \n",
    "                test_df, \n",
    "                glove_model,\n",
    "                embedding_size\n",
    "            )\n",
    "\n",
    "            # Procesar los datos y convertir los tokens en índices usando el modelo GloVe\n",
    "            X_train = prepare_data(train_df, processor_, word2vec_model=glove_model)\n",
    "            X_val = prepare_data(val_df, processor_, word2vec_model=glove_model)\n",
    "            X_test = prepare_data(test_df, processor_, word2vec_model=glove_model)\n",
    "\n",
    "            # Obtener la longitud máxima de secuencia\n",
    "            max_length = max(\n",
    "                max(len(x) for x in X_train),\n",
    "                max(len(x) for x in X_val),\n",
    "                max(len(x) for x in X_test)\n",
    "            )\n",
    "            print(f\"\\nMaximum sequence length for {glove_name}: {max_length}\")\n",
    "\n",
    "            # Pad sequences\n",
    "            print(f\"\\nPadding sequences for {glove_name}...\")\n",
    "            X_train_padded = pad_sequences(X_train, maxlen=max_length, padding='post')\n",
    "            X_val_padded = pad_sequences(X_val, maxlen=max_length, padding='post')\n",
    "            X_test_padded = pad_sequences(X_test, maxlen=max_length, padding='post')\n",
    "\n",
    "            # Preparar etiquetas (solo una vez)\n",
    "            print(\"\\nEncoding labels...\")\n",
    "            label_encoder = LabelEncoder()\n",
    "            label_encoder.fit(train_df['author'])\n",
    "            y_train = label_encoder.transform(train_df['author'])\n",
    "            y_val = label_encoder.transform(val_df['author'])\n",
    "            y_test = label_encoder.transform(test_df['author'])\n",
    "\n",
    "            # Obtener tamaño del vocabulario\n",
    "            vocab_size = len(glove_model.key_to_index) + 1\n",
    "            print(f\"\\nVocabulary size for {glove_name}: {vocab_size}\")\n",
    "\n",
    "            # Entrenar y evaluar los modelos\n",
    "            models = {\n",
    "                f'RDD1_{glove_name}': RDD1(embedding_matrix, vocab_size, embedding_size, max_length),\n",
    "                f'RDD2_{glove_name}': RDD2(embedding_matrix, vocab_size, embedding_size, max_length),\n",
    "                f'RDD3_{glove_name}': RDD3(embedding_matrix, vocab_size, embedding_size, max_length)\n",
    "            }\n",
    "\n",
    "            for model_name, model in models.items():\n",
    "                print(f\"\\n=== Training {model_name} ===\")\n",
    "                print(\"\\nModel Architecture:\")\n",
    "                model.summary()\n",
    "\n",
    "                # Entrenar el modelo\n",
    "                print(f\"\\nTraining model {model_name}...\")\n",
    "                history = train_model(\n",
    "                    model=model,\n",
    "                    train_data=(X_train_padded, y_train),\n",
    "                    val_data=(X_val_padded, y_val),\n",
    "                    batch_size=32,\n",
    "                    epochs=20\n",
    "                )\n",
    "\n",
    "                # Evaluar y almacenar resultados\n",
    "                accuracy, precision, recall = evaluate_model(\n",
    "                    model=model,\n",
    "                    test_data=(X_test_padded, y_test)\n",
    "                )\n",
    "\n",
    "                results[model_name] = {\n",
    "                    'val_accuracy': max(history.history['val_accuracy']),\n",
    "                    'train_accuracy': max(history.history['accuracy']),\n",
    "                    'test_accuracy': accuracy,\n",
    "                    'test_precision': precision,\n",
    "                    'test_recall': recall\n",
    "                }\n",
    "\n",
    "                # Imprimir resultados\n",
    "                print(f\"\\n{model_name} Results:\")\n",
    "                print(f\"Best validation accuracy: {results[model_name]['val_accuracy']:.4f}\")\n",
    "                print(f\"Best training accuracy: {results[model_name]['train_accuracy']:.4f}\")\n",
    "                print(f\"Test Accuracy: {results[model_name]['test_accuracy']:.4f}\")\n",
    "                print(f\"Test Precision: {results[model_name]['test_precision']:.4f}\")\n",
    "                print(f\"Test Recall: {results[model_name]['test_recall']:.4f}\")\n",
    "                \n",
    "                print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "        # Comparación final de resultados\n",
    "        print(\"\\n=== Final Model Comparison ===\")\n",
    "        comparison_df = pd.DataFrame(results).round(4)\n",
    "        print(comparison_df)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError in pipeline execution: {str(e)}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Starting Author Classification Pipeline with GloVe ===\n",
      "[==================================================] 100.0% 252.1/252.1MB downloaded\n",
      "\n",
      "1. Processing text data...\n",
      "\n",
      "Preparing embedding matrix and data for glove_200...\n",
      "Procesando textos...\n",
      "Procesando textos...\n",
      "Procesando textos...\n",
      "Longitud máxima de secuencia: 121\n",
      "Glove model detected\n",
      "Procesando textos...\n",
      "Procesando textos...\n",
      "Procesando textos...\n",
      "\n",
      "Maximum sequence length for glove_200: 121\n",
      "\n",
      "Padding sequences for glove_200...\n",
      "\n",
      "Encoding labels...\n",
      "\n",
      "Vocabulary size for glove_200: 400001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training RDD1_glove_200 ===\n",
      "\n",
      "Model Architecture:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">80,000,200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │    \u001b[38;5;34m80,000,200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">80,000,200</span> (305.18 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m80,000,200\u001b[0m (305.18 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">80,000,200</span> (305.18 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m80,000,200\u001b[0m (305.18 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model RDD1_glove_200...\n",
      "Epoch 1/20\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 46ms/step - accuracy: 0.4078 - loss: 1.3175 - val_accuracy: 0.4455 - val_loss: 0.9950\n",
      "Epoch 2/20\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 46ms/step - accuracy: 0.4932 - loss: 0.9487 - val_accuracy: 0.6162 - val_loss: 0.8449\n",
      "Epoch 3/20\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 44ms/step - accuracy: 0.5394 - loss: 0.8764 - val_accuracy: 0.6078 - val_loss: 0.8427\n",
      "Epoch 4/20\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 45ms/step - accuracy: 0.5632 - loss: 0.8387 - val_accuracy: 0.6027 - val_loss: 0.8327\n",
      "Epoch 5/20\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 47ms/step - accuracy: 0.5803 - loss: 0.8116 - val_accuracy: 0.6103 - val_loss: 0.8239\n",
      "Epoch 6/20\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 45ms/step - accuracy: 0.5831 - loss: 0.8011 - val_accuracy: 0.6255 - val_loss: 0.8414\n",
      "Epoch 7/20\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 44ms/step - accuracy: 0.5851 - loss: 0.7933 - val_accuracy: 0.5545 - val_loss: 0.8820\n",
      "Epoch 8/20\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 42ms/step - accuracy: 0.5831 - loss: 0.7857 - val_accuracy: 0.6044 - val_loss: 0.8311\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\n",
      "RDD1_glove_200 Results:\n",
      "Best validation accuracy: 0.6255\n",
      "Best training accuracy: 0.5837\n",
      "Test Accuracy: 0.6258\n",
      "Test Precision: 0.5119\n",
      "Test Recall: 0.6258\n",
      "\n",
      "==================================================\n",
      "\n",
      "=== Training RDD2_glove_200 ===\n",
      "\n",
      "Model Architecture:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">80,000,200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │    \u001b[38;5;34m80,000,200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">80,000,200</span> (305.18 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m80,000,200\u001b[0m (305.18 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">80,000,200</span> (305.18 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m80,000,200\u001b[0m (305.18 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model RDD2_glove_200...\n",
      "Epoch 1/20\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 44ms/step - accuracy: 0.3997 - loss: 1.3397 - val_accuracy: 0.4455 - val_loss: 0.9898\n",
      "Epoch 2/20\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 45ms/step - accuracy: 0.4458 - loss: 1.0227 - val_accuracy: 0.5841 - val_loss: 0.9257\n",
      "Epoch 3/20\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 42ms/step - accuracy: 0.4931 - loss: 0.9454 - val_accuracy: 0.5926 - val_loss: 0.8686\n",
      "Epoch 4/20\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 41ms/step - accuracy: 0.5671 - loss: 0.8725 - val_accuracy: 0.5495 - val_loss: 0.9352\n",
      "Epoch 5/20\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 41ms/step - accuracy: 0.5911 - loss: 0.8211 - val_accuracy: 0.6137 - val_loss: 0.8163\n",
      "Epoch 6/20\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 42ms/step - accuracy: 0.6126 - loss: 0.7767 - val_accuracy: 0.6255 - val_loss: 0.7937\n",
      "Epoch 7/20\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 44ms/step - accuracy: 0.6169 - loss: 0.7694 - val_accuracy: 0.6340 - val_loss: 0.8035\n",
      "Epoch 8/20\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 44ms/step - accuracy: 0.6242 - loss: 0.7436 - val_accuracy: 0.6086 - val_loss: 0.8333\n",
      "Epoch 9/20\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 43ms/step - accuracy: 0.6288 - loss: 0.7427 - val_accuracy: 0.6314 - val_loss: 0.8152\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\n",
      "RDD2_glove_200 Results:\n",
      "Best validation accuracy: 0.6340\n",
      "Best training accuracy: 0.6295\n",
      "Test Accuracy: 0.6398\n",
      "Test Precision: 0.5002\n",
      "Test Recall: 0.6398\n",
      "\n",
      "==================================================\n",
      "\n",
      "=== Training RDD3_glove_200 ===\n",
      "\n",
      "Model Architecture:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">121</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_2         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">121</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)  │ <span style=\"color: #00af00; text-decoration-color: #00af00\">80,000,200</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ spatial_dropout1d   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">121</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SpatialDropout1D</span>)  │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">121</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">935,936</span> │ spatial_dropout1… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalization │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">121</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ bidirectional[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">121</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">525,568</span> │ layer_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">121</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n",
       "│                     │                   │            │ multi_head_atten… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">121</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_1     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">121</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">656,384</span> │ layer_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">121</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ bidirectional_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_2     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">121</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">164,352</span> │ layer_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">121</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ bidirectional_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_average_p… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ global_average_p… │\n",
       "│                     │                   │            │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">459,264</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalization │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │ dense_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │ dropout_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ dense_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │ dropout_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ dense_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">387</span> │ dropout_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m121\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_2         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m121\u001b[0m, \u001b[38;5;34m200\u001b[0m)  │ \u001b[38;5;34m80,000,200\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ spatial_dropout1d   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m121\u001b[0m, \u001b[38;5;34m200\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ embedding_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│ (\u001b[38;5;33mSpatialDropout1D\u001b[0m)  │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m121\u001b[0m, \u001b[38;5;34m512\u001b[0m)  │    \u001b[38;5;34m935,936\u001b[0m │ spatial_dropout1… │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalization │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m121\u001b[0m, \u001b[38;5;34m512\u001b[0m)  │      \u001b[38;5;34m1,024\u001b[0m │ bidirectional[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m121\u001b[0m, \u001b[38;5;34m512\u001b[0m)  │    \u001b[38;5;34m525,568\u001b[0m │ layer_normalizat… │\n",
       "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add (\u001b[38;5;33mAdd\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m121\u001b[0m, \u001b[38;5;34m512\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n",
       "│                     │                   │            │ multi_head_atten… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m121\u001b[0m, \u001b[38;5;34m512\u001b[0m)  │      \u001b[38;5;34m1,024\u001b[0m │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_1     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m121\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │    \u001b[38;5;34m656,384\u001b[0m │ layer_normalizat… │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m121\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │        \u001b[38;5;34m512\u001b[0m │ bidirectional_1[\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_2     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m121\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │    \u001b[38;5;34m164,352\u001b[0m │ layer_normalizat… │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m121\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │        \u001b[38;5;34m256\u001b[0m │ bidirectional_2[\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n",
       "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n",
       "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n",
       "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m896\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ global_average_p… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ global_average_p… │\n",
       "│                     │                   │            │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │    \u001b[38;5;34m459,264\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalization │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │      \u001b[38;5;34m2,048\u001b[0m │ dense_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │    \u001b[38;5;34m131,328\u001b[0m │ dropout_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │      \u001b[38;5;34m1,024\u001b[0m │ dense_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m32,896\u001b[0m │ dropout_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m512\u001b[0m │ dense_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │        \u001b[38;5;34m387\u001b[0m │ dropout_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">82,912,715</span> (316.29 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m82,912,715\u001b[0m (316.29 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,910,723</span> (11.10 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,910,723\u001b[0m (11.10 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">80,001,992</span> (305.18 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m80,001,992\u001b[0m (305.18 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model RDD3_glove_200...\n",
      "Epoch 1/20\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 453ms/step - accuracy: 0.4653 - loss: 1.2568 - val_accuracy: 0.7447 - val_loss: 0.8059\n",
      "Epoch 2/20\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 450ms/step - accuracy: 0.7804 - loss: 0.5450 - val_accuracy: 0.8090 - val_loss: 0.4665\n",
      "Epoch 3/20\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 465ms/step - accuracy: 0.8280 - loss: 0.4254 - val_accuracy: 0.8639 - val_loss: 0.3857\n",
      "Epoch 4/20\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 464ms/step - accuracy: 0.8585 - loss: 0.3568 - val_accuracy: 0.8825 - val_loss: 0.2895\n",
      "Epoch 5/20\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 450ms/step - accuracy: 0.8645 - loss: 0.3335 - val_accuracy: 0.8707 - val_loss: 0.3067\n",
      "Epoch 6/20\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 451ms/step - accuracy: 0.8862 - loss: 0.2874 - val_accuracy: 0.9011 - val_loss: 0.2321\n",
      "Epoch 7/20\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 450ms/step - accuracy: 0.8994 - loss: 0.2675 - val_accuracy: 0.9189 - val_loss: 0.2152\n",
      "Epoch 8/20\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 451ms/step - accuracy: 0.9033 - loss: 0.2449 - val_accuracy: 0.9138 - val_loss: 0.2221\n",
      "Epoch 9/20\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 464ms/step - accuracy: 0.9106 - loss: 0.2344 - val_accuracy: 0.9273 - val_loss: 0.1895\n",
      "Epoch 10/20\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 465ms/step - accuracy: 0.9154 - loss: 0.2180 - val_accuracy: 0.9273 - val_loss: 0.1844\n",
      "Epoch 11/20\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 463ms/step - accuracy: 0.9210 - loss: 0.2059 - val_accuracy: 0.9256 - val_loss: 0.1983\n",
      "Epoch 12/20\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 443ms/step - accuracy: 0.9340 - loss: 0.1800 - val_accuracy: 0.9231 - val_loss: 0.1799\n",
      "Epoch 13/20\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 471ms/step - accuracy: 0.9379 - loss: 0.1715 - val_accuracy: 0.9248 - val_loss: 0.1890\n",
      "Epoch 14/20\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 480ms/step - accuracy: 0.9359 - loss: 0.1637 - val_accuracy: 0.9341 - val_loss: 0.2066\n",
      "Epoch 15/20\n",
      "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 466ms/step - accuracy: 0.9459 - loss: 0.1471 - val_accuracy: 0.9248 - val_loss: 0.1918\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 140ms/step\n",
      "\n",
      "RDD3_glove_200 Results:\n",
      "Best validation accuracy: 0.9341\n",
      "Best training accuracy: 0.9433\n",
      "Test Accuracy: 0.9300\n",
      "Test Precision: 0.9319\n",
      "Test Recall: 0.9300\n",
      "\n",
      "==================================================\n",
      "\n",
      "=== Final Model Comparison ===\n",
      "                RDD1_glove_200  RDD2_glove_200  RDD3_glove_200\n",
      "val_accuracy            0.6255          0.6340          0.9341\n",
      "train_accuracy          0.5837          0.6295          0.9433\n",
      "test_accuracy           0.6258          0.6398          0.9300\n",
      "test_precision          0.5119          0.5002          0.9319\n",
      "test_recall             0.6258          0.6398          0.9300\n"
     ]
    }
   ],
   "source": [
    "run_classification_pipeline_glove()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
