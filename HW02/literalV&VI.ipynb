{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importación de bibliotecas, funciones necesarias y carga de modelos\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "import pickle\n",
    "import math\n",
    "import random\n",
    "\n",
    "def crearDefaultdictCounter():\n",
    "    \"\"\"\n",
    "    Crea un defaultdict de Counter, para utilizar en el modelo de trigramas\n",
    "    \n",
    "    Retorna:\n",
    "    defaultdict(Counter): Un defaultdict que retorna un Counter por defecto\n",
    "    \"\"\"\n",
    "    return defaultdict(Counter)\n",
    "\n",
    "def cargarModelo(nombreArchivo):\n",
    "    \"\"\"\n",
    "    Carga un modelo desde un archivo pickle.\n",
    "    \n",
    "    Parámetros:\n",
    "    - nombreArchivo (str): Nombre del archivo desde donde se cargará el modelo\n",
    "    \n",
    "    Retorna:\n",
    "    - modelo: El modelo cargado.\n",
    "    \"\"\"\n",
    "    with open(nombreArchivo, 'rb') as archivo:\n",
    "        modelo = pickle.load(archivo)\n",
    "    return modelo\n",
    "\n",
    "def separarOraciones(datos):\n",
    "    \"\"\"\n",
    "    Separar las oraciones de un DataFrame a partir de la columna 'unique'\n",
    "    \n",
    "    Parámetros:\n",
    "    - datos (pd.DataFrame): DataFrame que contiene una columna 'unique' con las oraciones\n",
    "    \n",
    "    Retorna\n",
    "    - list: Lista de oraciones tokenizadas, donde cada oración es una lista de tokens\n",
    "    \"\"\"\n",
    "    oracionesSeparadas = []\n",
    "    for oracion in datos['unique']:\n",
    "        tokens = oracion.split()  \n",
    "        oracionesSeparadas.append(tokens)\n",
    "    return oracionesSeparadas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#20N\n",
    "unigrama20N = cargarModelo('20N_01_unigramas.pkl')\n",
    "bigrama20N = cargarModelo('20N_01_bigramas.pkl')\n",
    "trigrama20N = cargarModelo('20N_01_trigramas.pkl')\n",
    "\n",
    "#BAC\n",
    "unigramaBAC = cargarModelo('BAC_01_unigramas.pkl')\n",
    "bigramaBAC = cargarModelo('BAC_01_bigramas.pkl')\n",
    "trigramaBAC = cargarModelo('BAC_01_trigramas.pkl')\n",
    "\n",
    "# testing csvs\n",
    "datos20N_test = pd.read_csv('20N_01_testing.csv')\n",
    "datosBAC_test = pd.read_csv('BAC_01_testing.csv')\n",
    "oracionesSeparadas20N_test = separarOraciones(datos20N_test)\n",
    "oracionesSeparadasBAC_test = separarOraciones(datosBAC_test)\n",
    "\n",
    "tamanoVocabulario20N = len(unigrama20N)\n",
    "tamanoVocabularioBAC = len(unigramaBAC)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cálculo de Perplejidad para los Modelos N-Grama\n",
    "\n",
    "Para wvaluar el rendimiento de nuestros modelos de lenguaje, calculamos la **perplejidad** de cada uno sobre el conjunto de prueba. La perplejidad es una medida que indica qué tan bien un modelo predice una muestra. Un valor de perplejidad más bajo sugiere un modelo mejor\n",
    "\n",
    "Definimos funciones para calcular la perplejidad de los modelos de **unigramas**, **bigramas** y **trigramas**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcularPerplejidadUnigrama(modeloUnigrama, oracionesSeparadas, tamanoVocabulario):\n",
    "    \"\"\"\n",
    "    Calcula la perplejidad de un modelo de unigramas dado un conjunto de oraciones tokenizadas\n",
    "\n",
    "    Parámetros:\n",
    "    - modeloUnigrama (Counter): Modelo de unigramas\n",
    "    - oracionesSeparadas (list): Lista de oraciones tokenizadas\n",
    "    - tamanoVocabulario (int): Tamaño del vocabulario del modelo\n",
    "\n",
    "    Retorna:\n",
    "    - float: Perplejidad del modelo sobre el conjunto de oracione\n",
    "    \"\"\"\n",
    "    totalLogProb = 0\n",
    "    totalPalabras = 0\n",
    "    totalFrecuencia = sum(modeloUnigrama.values())\n",
    "\n",
    "    for oracion in oracionesSeparadas:\n",
    "        for palabra in oracion:\n",
    "            frecuencia = modeloUnigrama.get(palabra, 0)\n",
    "            probabilidad = (frecuencia + 1) / (totalFrecuencia + tamanoVocabulario)\n",
    "            logProb = math.log(probabilidad)\n",
    "            totalLogProb += logProb\n",
    "            totalPalabras += 1\n",
    "\n",
    "    perplejidad = math.exp(-totalLogProb / totalPalabras)\n",
    "    return perplejidad\n",
    "\n",
    "def calcularPerplejidadBigrama(modeloBigrama, modeloUnigrama, oracionesSeparadas, tamanoVocabulario):\n",
    "    \"\"\"\n",
    "    Calcula la perplejidad de un modelo de bigramas dado un conjunto de oraciones tokenizadas\n",
    "\n",
    "    Parámetros:\n",
    "    - modeloBigrama (defaultdict(Counter)): Modelo de bigramas\n",
    "    - modeloUnigrama (Counter): Modelo de unigramas\n",
    "    - oracionesSeparadas (list): Lista de oraciones tokenizqdas\n",
    "    - tamanoVocabulario (int): Tamaño del vocabulario del modelo\n",
    "\n",
    "    Retorna:\n",
    "    - float: Perplejidad del modelo sobre el conjunto de oraciones\n",
    "    \"\"\"\n",
    "    totalLogProb = 0\n",
    "    totalPalabras = 0\n",
    "\n",
    "    for oracion in oracionesSeparadas:\n",
    "        for i in range(len(oracion) - 1):\n",
    "            palabra1 = oracion[i]\n",
    "            palabra2 = oracion[i+1]\n",
    "\n",
    "            frecuenciaBigram = modeloBigrama.get(palabra1, {}).get(palabra2, 0)\n",
    "            frecuenciaUnigram = modeloUnigrama.get(palabra1, 0)\n",
    "            probabilidad = (frecuenciaBigram + 1) / (frecuenciaUnigram + tamanoVocabulario)\n",
    "            logProb = math.log(probabilidad)\n",
    "            totalLogProb += logProb\n",
    "            totalPalabras += 1\n",
    "\n",
    "    perplejidad = math.exp(-totalLogProb / totalPalabras)\n",
    "    return perplejidad\n",
    "\n",
    "def calcularPerplejidadTrigrama(modeloTrigrama, modeloBigrama, oracionesSeparadas, tamanoVocabulario):\n",
    "    \"\"\"\n",
    "    Calcula la perplejidad de un modelo de trigramas dado un conjunto de oraciones tokenizadas\n",
    "\n",
    "    Parámetros:\n",
    "    - modeloTrigrama (defaultdict): Modelo de trigramas\n",
    "    - modeloBigrama (defaultdict(Counter)): Modelo de bigramas\n",
    "    - oracionesSeparadas (list): Lista de oraciones tokenizadas\n",
    "\n",
    "    Retorna:\n",
    "    - float: Perplejidad del modelo sobre el conjunto de oraciones\n",
    "    \"\"\"\n",
    "    totalLogProb = 0\n",
    "    totalPalabras = 0\n",
    "\n",
    "    for oracion in oracionesSeparadas:\n",
    "        for i in range(len(oracion) - 2):\n",
    "            palabra1 = oracion[i]\n",
    "            palabra2 = oracion[i+1]\n",
    "            palabra3 = oracion[i+2]\n",
    "\n",
    "            frecuenciaTrigram = modeloTrigrama.get(palabra1, {}).get(palabra2, {}).get(palabra3, 0)\n",
    "            frecuenciaBigram = modeloBigrama.get(palabra1, {}).get(palabra2, 0)\n",
    "            probabilidad = (frecuenciaTrigram + 1) / (frecuenciaBigram + tamanoVocabulario)\n",
    "            logProb = math.log(probabilidad)\n",
    "            totalLogProb += logProb\n",
    "            totalPalabras += 1\n",
    "\n",
    "    perplejidad = math.exp(-totalLogProb / totalPalabras)\n",
    "    return perplejidad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultados de la Perplejidad\n",
    "\n",
    "Después de calcular las perplejidades, organizamos los resultados en una tabla \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplejidades calculadas:\n",
      "\n",
      "Modelo Unigrama 20N: 1514.136822471852\n",
      "Modelo Bigrama 20N: 3119.2630248796004\n",
      "Modelo Trigrama 20N: 18635.35423630719\n",
      "\n",
      "Modelo Unigrama BAC: 1111.34977853784\n",
      "Modelo Bigrama BAC: 1399.9012431670694\n",
      "Modelo Trigrama BAC: 20575.56431957699\n"
     ]
    }
   ],
   "source": [
    "# Calcular perplejidad para 20N\n",
    "perplejidadUnigrama20N = calcularPerplejidadUnigrama(unigrama20N, oracionesSeparadas20N_test, tamanoVocabulario20N)\n",
    "perplejidadBigrama20N = calcularPerplejidadBigrama(bigrama20N, unigrama20N, oracionesSeparadas20N_test, tamanoVocabulario20N)\n",
    "perplejidadTrigrama20N = calcularPerplejidadTrigrama(trigrama20N, bigrama20N, oracionesSeparadas20N_test, tamanoVocabulario20N)\n",
    "\n",
    "# Calcular perplejidad para BAC\n",
    "perplejidadUnigramaBAC = calcularPerplejidadUnigrama(unigramaBAC, oracionesSeparadasBAC_test, tamanoVocabularioBAC)\n",
    "perplejidadBigramaBAC = calcularPerplejidadBigrama(bigramaBAC, unigramaBAC, oracionesSeparadasBAC_test, tamanoVocabularioBAC)\n",
    "perplejidadTrigramaBAC = calcularPerplejidadTrigrama(trigramaBAC, bigramaBAC, oracionesSeparadasBAC_test, tamanoVocabularioBAC)\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"Perplejidades calculadas:\\n\")\n",
    "print(f\"Modelo Unigrama 20N: {perplejidadUnigrama20N}\")\n",
    "print(f\"Modelo Bigrama 20N: {perplejidadBigrama20N}\")\n",
    "print(f\"Modelo Trigrama 20N: {perplejidadTrigrama20N}\\n\")\n",
    "\n",
    "print(f\"Modelo Unigrama BAC: {perplejidadUnigramaBAC}\")\n",
    "print(f\"Modelo Bigrama BAC: {perplejidadBigramaBAC}\")\n",
    "print(f\"Modelo Trigrama BAC: {perplejidadTrigramaBAC}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabla de perplejidades:\n",
      "\n",
      "     Modelo  Perplejidad 20N  Perplejidad BAC\n",
      "0  Unigrama      1514.136822      1111.349779\n",
      "1   Bigrama      3119.263025      1399.901243\n",
      "2  Trigrama     18635.354236     20575.564320\n"
     ]
    }
   ],
   "source": [
    "resultados = pd.DataFrame({\n",
    "    'Modelo': ['Unigrama', 'Bigrama', 'Trigrama'],\n",
    "    'Perplejidad 20N': [perplejidadUnigrama20N, perplejidadBigrama20N, perplejidadTrigrama20N],\n",
    "    'Perplejidad BAC': [perplejidadUnigramaBAC, perplejidadBigramaBAC, perplejidadTrigramaBAC]\n",
    "})\n",
    "\n",
    "print(\"Tabla de perplejidades:\\n\")\n",
    "print(resultados)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- El **modelo de unigramas** tiene la perplejidad más baja en ambos datasets, lo que indica que, desde una perspectiva de predicción, es el más efectivo según esta métrica\n",
    "- Los **modelos de bigramas y trigramas** presentan perplejidades más altas, posiblemente debido a la escasez de datos para ciertas combinaciones de palabras\n",
    "\n",
    "Sin embargo, es importante considerar que aunque el modelo de unigramas tiene la perplejidad más baja no captura las dependencias entre palabras, lo cual es esencial para generar oraciones coherentes!!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generación de Oraciones Utilizando el Mejor Modelo (Modelo Unigrama)\n",
    "\n",
    "De acuerdo con los resultados obtenidos, el **modelo de unigramas** presenta la perplejidad más baja en ambos datasets, por lo que es considerado el mejor modelo según esta métrica.\n",
    "\n",
    "Aunque los modelos de unigramas no capturan las dependencias entre palabras, utilizaremos este modelo para generar oraciones porque lo dicen en elenunciado\n",
    "\n",
    "Definimos una función que, dado un modelo de unigramas y una palabra inicial, genera una oración de manera automática.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generarSentenciaUnigrama(modeloUnigrama, palabraInicial, tamanoVocabulario, maxLongitud=20):\n",
    "    \"\"\"\n",
    "    Genera una oración utilizando el modelo de unigramas, comenzando con una palabra dada\n",
    "    \n",
    "    Parámetros:\n",
    "    - modeloUnigrama (Counter): Modelo de unigramas\n",
    "    - palabraInicial (str): Palabra inicial de la oración\n",
    "    - tamanoVocabulario (int): Tamaño del vocabulario\n",
    "    - maxLongitud (int): Longitud máxima de la oración generad\n",
    "    \n",
    "    Retorna:\n",
    "    - str: Oración generada.\n",
    "    \"\"\"\n",
    "    oracion = [palabraInicial]\n",
    "    \n",
    "    totalFrecuencia = sum(modeloUnigrama.values()) + tamanoVocabulario\n",
    "    palabras = list(modeloUnigrama.keys())\n",
    "    frecuencias = [modeloUnigrama.get(palabra, 0) + 1 for palabra in palabras]  \n",
    "    \n",
    "    probabilidades = np.array(frecuencias) / totalFrecuencia\n",
    "    \n",
    "    for _ in range(maxLongitud - 1):\n",
    "        palabraSiguiente = np.random.choice(palabras, p=probabilidades)\n",
    "        oracion.append(palabraSiguiente)\n",
    "        \n",
    "        # Si la palabra es el token de fin de oración, detenemos la generación\n",
    "        if palabraSiguiente == '</s>':\n",
    "            break\n",
    "    \n",
    "    return ' '.join(oracion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruebas de Generación de Oraciones y Análisis de Resultados\n",
    "\n",
    "Probamos la función de generación de oraciones con diferentes palabras iniciales para evaluar cómo el modelo de unigramas genera texto. Algunas de las palabras iniciales utilizadas son:\n",
    "\n",
    "- `<s>` (inicio de oración)\n",
    "- `the`\n",
    "- `NUM`\n",
    "- `this`\n",
    "- `it`\n",
    "- `i`\n",
    "- `he`\n",
    "- `she`\n",
    "- `they`\n",
    "- `we`\n",
    "\n",
    "Mostramos las oraciones generadas y analizamos su coherencia y características.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generación de oraciones utilizando el modelo Unigrama:\n",
      "\n",
      "Palabra inicial: '<s>'\n",
      "Oración generada: <s> webb to hope motivation net would you the NUM a come credit edu intermediate and medical a of powerbook\n",
      "\n",
      "Palabra inicial: 'the'\n",
      "Oración generada: the getting is of this hemorrhage the react the ut but so the final key and article or keresh NUM\n",
      "\n",
      "Palabra inicial: 'NUM'\n",
      "Oración generada: NUM in we ax of tries do to i y leagues left number for dislike the me did a additionally\n",
      "\n",
      "Palabra inicial: 'this'\n",
      "Oración generada: this you emulates NUM check reported modifiable intended via j3f end do as afraid though blue mb or NUM at\n",
      "\n",
      "Palabra inicial: 'it'\n",
      "Oración generada: it robbery persuasive move of f NUM up disk NUM this credited activists radio what some NUM good appeal linked\n",
      "\n",
      "Palabra inicial: 'i'\n",
      "Oración generada: i elderly the l5 much to information stuff all the NUM native etc continue xtappaddtimeout with adapter diesels you corvette\n",
      "\n",
      "Palabra inicial: 'he'\n",
      "Oración generada: he world being multiple really those chemistry provide a animal devloping que <s> bringwindowtotop NUM NUM are c5sqv8 both like\n",
      "\n",
      "Palabra inicial: 'she'\n",
      "Oración generada: she file under bmug list quite fluids NUM is ax s a lines ideas only 6e of forsrg out which\n",
      "\n",
      "Palabra inicial: 'they'\n",
      "Oración generada: they <UNK> ax are but written different cc government well toronto gcc ted dated john berkeley NUM mkg3states capability NUM\n",
      "\n",
      "Palabra inicial: 'we'\n",
      "Oración generada: we in the shots just us an bitlis answer _ like dubious ax the where not u planned friend never\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lista de palabras iniciales para probar\n",
    "palabrasIniciales = ['<s>', 'the', 'NUM', 'this', 'it', 'i', 'he', 'she', 'they', 'we']\n",
    "\n",
    "print(\"Generación de oraciones utilizando el modelo Unigrama:\\n\")\n",
    "\n",
    "for palabraInicial in palabrasIniciales:\n",
    "    if palabraInicial not in unigrama20N:\n",
    "        palabraInicial = '<UNK>'\n",
    "    \n",
    "    oracionGenerada = generarSentenciaUnigrama(unigrama20N, palabraInicial, tamanoVocabulario20N)\n",
    "    \n",
    "    print(f\"Palabra inicial: '{palabraInicial}'\")\n",
    "    print(f\"Oración generada: {oracionGenerada}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis de las Oraciones Generadas\n",
    "\n",
    "Al revisar las oraciones generadas por el **modelo de unigramas**, observamos que:\n",
    "\n",
    "- Las palabras aparecen de manera independiente, sin considerar el contexto previo\n",
    "- Las oraciones suelen no tener de coherencia gramatical y semántica\n",
    "- Es común que las oraciones sean simplemente secuencias aleatorias de palabras del vocabulario, reflejando las frecuencias individuales de las palabras en el corpus.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generación de Oraciones Utilizando el Modelo Trigrama\n",
    "\n",
    "Con el objetivo de mejorar la coherencia y contexto de las oraciones generadas, implementamos una función para generar oraciones utilizando el **modelo de trigramas**. Al considerar dos palabras previas, el modelo trigrama puede capturar dependencias más largas.\n",
    "\n",
    "Realizamos pruebas similares a las anteriores y comparamos los resultados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generarSentenciaTrigrama(modeloTrigrama, modeloBigrama, palabraInicial, tamanoVocabulario, maxLongitud=20):\n",
    "    \"\"\"\n",
    "    Genera una oración utilizando el modelo de trigramas, comenzando con una palabra dada\n",
    "\n",
    "    Parámetros:\n",
    "    - modeloTrigrama (defaultdict): Modelo de trigramqs\n",
    "    - modeloBigrama (defaultdict(Counter)): Modelo de bigramas\n",
    "    - palabraInicial (str): Palabra inicial de la oración\n",
    "    - tamanoVocabulario (int): Tamaño del vocabulario\n",
    "    - maxLongitud (int): Longitud máxima de la oración generad\n",
    "\n",
    "    Retorna:\n",
    "    - str: Oración generada.\n",
    "    \"\"\"\n",
    "    oracion = [palabraInicial]\n",
    "\n",
    "    posiblesSiguientes = modeloBigrama.get(palabraInicial, None)\n",
    "    if not posiblesSiguientes:\n",
    "        return ' '.join(oracion)\n",
    "\n",
    "    totalFrecuencia = sum(posiblesSiguientes.values()) + tamanoVocabulario\n",
    "    palabras = []\n",
    "    probabilidades = []\n",
    "\n",
    "    for palabra, frecuencia in posiblesSiguientes.items():\n",
    "        palabras.append(palabra)\n",
    "        probabilidad = (frecuencia + 1) / totalFrecuencia\n",
    "        probabilidades.append(probabilidad)\n",
    "\n",
    "    probabilidades = np.array(probabilidades)\n",
    "    probabilidades /= probabilidades.sum()\n",
    "\n",
    "    palabraSiguiente = np.random.choice(palabras, p=probabilidades)\n",
    "    oracion.append(palabraSiguiente)\n",
    "\n",
    "    for _ in range(maxLongitud - 2):\n",
    "        palabra1 = oracion[-2]\n",
    "        palabra2 = oracion[-1]\n",
    "        posiblesSiguientes = modeloTrigrama.get(palabra1, {}).get(palabra2, None)\n",
    "\n",
    "        if not posiblesSiguientes:\n",
    "            break\n",
    "\n",
    "        totalFrecuencia = sum(posiblesSiguientes.values()) + tamanoVocabulario\n",
    "        palabras = []\n",
    "        probabilidades = []\n",
    "\n",
    "        for palabra, frecuencia in posiblesSiguientes.items():\n",
    "            palabras.append(palabra)\n",
    "            probabilidad = (frecuencia + 1) / totalFrecuencia\n",
    "            probabilidades.append(probabilidad)\n",
    "\n",
    "        probabilidades = np.array(probabilidades)\n",
    "        probabilidades /= probabilidades.sum()\n",
    "\n",
    "        palabraSiguiente = np.random.choice(palabras, p=probabilidades)\n",
    "        oracion.append(palabraSiguiente)\n",
    "\n",
    "        # Si la palabra es el token de fin de oración, detenemos la generación\n",
    "        if palabraSiguiente == '</s>':\n",
    "            break\n",
    "\n",
    "    return ' '.join(oracion)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generación de oraciones utilizando el modelo Trigrama:\n",
      "\n",
      "Palabra inicial: '<s>'\n",
      "Oración generada: <s> for sale NUM or NUM key layout resembles the u s said that it is the phase magnitude data\n",
      "\n",
      "Palabra inicial: 'the'\n",
      "Oración generada: the NUM addressing modes risc instruction sets and which has a library layered on top of i think i was\n",
      "\n",
      "Palabra inicial: 'NUM'\n",
      "Oración generada: NUM NUM NUM the government had a couple groups up there and do some more q to follow we can\n",
      "\n",
      "Palabra inicial: 'this'\n",
      "Oración generada: this is done it before your are devious by nature <UNK> noteworthy is the cubs williams had moved to pa\n",
      "\n",
      "Palabra inicial: 'it'\n",
      "Oración generada: it s NUM place to hide NUM s there but mike lawson public relations since the son and the site\n",
      "\n",
      "Palabra inicial: 'i'\n",
      "Oración generada: i use soft pc to run on unix machines and ibm <UNK> contact supports pc s perform great signs and\n",
      "\n",
      "Palabra inicial: 'he'\n",
      "Oración generada: he failed to appear politically correct plo have at hand robert weiss articles did you say god has not surprisingly\n",
      "\n",
      "Palabra inicial: 'she'\n",
      "Oración generada: she fell down and get the whole internet user s display hardware furthermore you will are very concerned that quick\n",
      "\n",
      "Palabra inicial: 'they'\n",
      "Oración generada: they market the older cars alt autos karting for the sale so it s not in ucb s got castrol\n",
      "\n",
      "Palabra inicial: 'we'\n",
      "Oración generada: we have an almost identical game to judge people judge whether you should change if they had been chosen the\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Generación de oraciones utilizando el modelo Trigrama:\\n\")\n",
    "\n",
    "for palabraInicial in palabrasIniciales:\n",
    "    if palabraInicial not in unigrama20N:\n",
    "        palabraInicial = '<UNK>'\n",
    "\n",
    "    oracionGenerada = generarSentenciaTrigrama(trigrama20N, bigrama20N, palabraInicial, tamanoVocabulario20N)\n",
    "\n",
    "    print(f\"Palabra inicial: '{palabraInicial}'\")\n",
    "    print(f\"Oración generada: {oracionGenerada}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis de las Oraciones Generadas con el Modelo Trigrama\n",
    "\n",
    "Las oraciones generadas con el modelo trigrama muestran:\n",
    "\n",
    "- Una ligerq mejora en la coherencia, gracias a la consideración de un contexto más amplio\n",
    "- Persistencia de algunas limitaciones, como repeticiones y posibles incoherencias\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
